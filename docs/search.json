[
  {
    "objectID": "posts/aperture_filtering/2021-12-08-Microscopy-Automatic-Aperture-Filtering.html",
    "href": "posts/aperture_filtering/2021-12-08-Microscopy-Automatic-Aperture-Filtering.html",
    "title": "Automatic Aperture Filtering for Microscopy Using Shapes",
    "section": "",
    "text": "The aperture/pupil in the back focal plane (BFP) of a microscope determines the resolution of the image that can be captured by the microscope. For a microscope, the aperture is basically the tube inside the microscope through which light passes. The BFP exists in Fourier space, and the aperture essentially blocks spatial frequencies beyond its width at the BFP. In the corresponding image at the camera, any spatial frequencies beyond the aperture width in the BFP cannot possibly have been made by the sample, because they were blocked by the aperture from getting to the camera. The goal is to utilize this understanding of the inner workings of the microscope to denoise a microscopy image by accounting for the aperture/pupil in the BFP.\nWe remove high-frequency noise that is beyond the aperture by modeling the pupil function of the image (i.e., the aperture) as a big circle and removing frequencies in the image outside of the aperture. By Fourier transforming the image, we expect to see non-zero power from spatial frequencies out to some radius away from the center – that circle is the aperture. Beyond this circle, the power should be zero, based on our physical understanding of a microscope.\nIn this example, we vary the radius of the circle to model the Fourier transformed image as zeros outside of the circle, and some number greater than zero inside the circle. Once the best pupil function is found, i.e. the one whose shape best matches the experimentally observed Fourier image, any spatial frequencies in the image beyond this point are zeroed out to low-pass filter that noise out of the image.\nWe’re using the Rat Hippocampal Neuron .tif file example from FIJI, which has several color channels in this notebook.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n## Open the file. It was opened from FIJI and then File > save as > Tif\n\nfrom skimage import io\nimg0 = io.imread('Rat_Hippocampal_Neuron.tif').astype('double') ## from FIJI examples\n\n## Use the DIC image to find the aperture\nimg = img0[-1]\n\n## Plot image\nfig,ax=plt.subplots(1,1,figsize=(6,6))\nax.imshow(img,cmap='Greys_r',origin='lower',vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std())\nax.set_title('Real Space')\nplt.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nplt.show()\n\n\n\n\n\n## Calculate various Fourier transform things\nft = np.fft.fftshift(np.fft.fft2(img))\nmag_img = np.sqrt(ft.real**2. + ft.imag**2.)\nphase_img = np.unwrap(np.arctan(ft.imag/ft.real))\nln_mag = np.log(mag_img)\n\n\n## Plot the magnitude and phase of the Fourier transform of the image\nfig,ax=plt.subplots(1,2,figsize=(12,6))\nvmin,vmax = np.percentile(ln_mag,(1,99))\nax[0].imshow(ln_mag,cmap='Greys_r',origin='lower',vmin=vmin,vmax=vmax,\n             extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2. ])\nax[0].set_title('Fourier Space (ln(Magnitude))')\nax[1].imshow(phase_img,cmap='Greys_r',origin='lower',\n            extent=[-phase_img.shape[1]/2.,phase_img.shape[1]/2., -phase_img.shape[0]/2., phase_img.shape[0]/2. ])\nax[1].set_title('Fourier Space (Phase)')\nplt.show()\n\n\n\n\nThis evidence function calculates a circular mask/template given a radius, r0. Any pixels within r0 have value 1. and any outside r0 have value 0. It’s like a 2D tophat function.\nThe evidence for this template is for m>0, b in R, tau>0 (SI 2.2.2). It is compared against the evidence for a flat or ‘Null’ template (i.e., m=0, b in R, tau >0 (SI 2.2.4).\nThis function returns the (negative) ratio of those two evidence functions so that the minimzer functions in scipy.optimize can find the maximum.\n\n## pre-computed mask calculation parameters\nnx,ny = ft.shape\nkx,ky = np.mgrid[0:nx,0:ny]\nkx = kx.astype('double') - nx/2.\nky = ky.astype('double') - ny/2.\nkr2 = kx**2. + ky**2.\n\nfrom scipy.special import betainc,betaln,gammaln\ndef ln_bayes_factor(theta,y):\n    r0 = theta\n    ## model is out of bounds\n    if r0 < 5:\n        return np.inf\n\n    ## make the template \n    x = (kr2 < r0**2.).astype('double')\n    N=float(x.size)\n\n    ex = np.nanmean(x)\n    exx = np.nanmean(x*x)\n    ey = np.nanmean(y)\n    eyy = np.nanmean(y*y)\n    exy = np.nanmean(x*y)\n    vx = exx - ex*ex + 1e-300\n    vy = eyy - ey*ey + 1e-300\n    vxy = exy - ex*ey + 1e-300\n    r = vxy/np.sqrt(vx*vy)\n    r2 = r*r\n    if r2 < 1e-10 or r2 > 1.-1e-10:\n        return np.inf\n    M = (N-2.)/2.\n\n    delm = 1e30\n    lnR = np.log(2.) + np.log(delm) - betaln(.5,M) + .5*np.log(vx) - .5*np.log(vy) + M*np.log(1.-r2) - np.log(1.+np.sign(r)*betainc(.5,M,r2))\n\n    return lnR\n\nprint(ln_bayes_factor((50.),ln_mag))\n\n-13934.824440777767\n\n\n\n### Scan the r0 parameter to see if there is a maximum\n\nxs = np.linspace(10,400,1000)*1.\nev = np.zeros_like(xs)\nfrom tqdm.notebook import trange\nfor i in trange(xs.size):\n    ev[i] = -ln_bayes_factor((xs[i]),ln_mag)\n    \nfig,ax = plt.subplots(1,figsize=(12,4))\nax.plot(xs,ev)\nplt.show()\n\n\n\n\n\n\n\n\n### Find the maximum prob aperture mask radius\n\nfrom scipy.optimize import minimize\n\ndef wrapper(initial_guess,ln_mag):\n    return -1./(1.+np.exp(ln_bayes_factor(initial_guess,ln_mag)))\n    \ninitial_guess = np.array((xs[np.nanargmax(ev)]))\nout = minimize(wrapper, initial_guess, args=ln_mag, method='Nelder-Mead')\nprint(out)\n\nr0 = out.x\nmask = (kr2 < r0**2.).astype('int')\n\n final_simplex: (array([[138.43843844],\n       [138.43849125]]), array([-1., -1.]))\n           fun: -1.0\n       message: 'Optimization terminated successfully.'\n          nfev: 53\n           nit: 18\n        status: 0\n       success: True\n             x: array([138.43843844])\n\n\n\n### Plot the best mask\nfig,ax=plt.subplots(1,2,figsize=(12,6))\nax[0].imshow(ln_mag,cmap='Greys_r',origin='lower',vmin=vmin,vmax=vmax,\n            extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2.])\nax[1].imshow(ln_mag*mask,cmap='Greys_r',origin='lower',vmin=vmin,vmax=vmax,\n            extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2.])\nax[0].set_title('Fourier Space')\nax[1].set_title('Fourier Space (Masked)')\nplt.show()\n\n\n\n\n\n### Calculate and Plot the low pass filtered image\nfiltered = ft*(mask+mask*1j)\nfiltered = np.fft.ifft2(np.fft.fftshift(filtered)).real\nresidual = img-filtered\n\nfig,ax=plt.subplots(3,2,figsize=(12,18))\nax[0,0].imshow(img,cmap='Greys_r',origin='lower',vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std())\nax[0,1].imshow(filtered,cmap='Greys_r',origin='lower',vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std())\nax[1,0].imshow(img,cmap='Greys_r',origin='lower',vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std())\nax[1,1].imshow(filtered,cmap='Greys_r',origin='lower',vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std())\nax[2,0].hist(residual.flatten(),bins=250,log=True)\nax[2,1].imshow(residual,cmap='Greys_r',origin='lower',vmin=np.percentile(residual,35),vmax=np.percentile(residual,65))\n\nfor aa in ax[1]:\n    aa.set_xlim(64,128)\n    aa.set_ylim(64,128)\n\nax[0,0].set_title('Real Space')\nax[0,0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[0,1].set_title('Real Space (Filtered)')\nax[0,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[1,0].set_title('Real Space (Zoom)')\nax[1,0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[1,1].set_title('Real Space (Filtered, Zoom)')\nax[1,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[2,0].set_title('Residual')\nax[2,1].set_title('Residual (Accentuated)')\nax[2,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\n\nplt.show()\n\n\n\n\n\n### Show the original and filtered image (first three fluorescence channels) in RGB\nfiltered = np.zeros((nx,ny,3))\nfull = np.zeros_like(filtered)\nfor i in range(3):\n    ft = np.fft.fftshift(np.fft.fft2(img0[i]))\n    fd = ft*(mask+mask*1j)\n    fd = np.fft.ifft2(np.fft.fftshift(fd)).real\n    filtered[:,:,i] = fd\n    full[:,:,i] = img0[i]\n\nscaling = full.max((0,1))[None,None,:]\nfiltered /= scaling\nfull /= scaling\n\nfig,ax = plt.subplots(1,2,figsize=(16,8))\nax[0].imshow(full,origin='lower',interpolation='nearest')\nax[0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[1].imshow(filtered,origin='lower',interpolation='nearest')\nax[1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[0].set_title('Original')\nax[1].set_title('Filtered')\nplt.show()\n\nfig,ax = plt.subplots(1,2,figsize=(16,8))\nax[0].imshow(full,origin='lower',interpolation='nearest')\nax[1].imshow(filtered,origin='lower',interpolation='nearest')\nax[0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\nax[0].set_title('Original (Zoom)')\nax[1].set_title('Filtered (Zoom)')\nfor aa in ax:\n    aa.set_xlim(200,328)\n    aa.set_ylim(200,328)\nplt.show()"
  },
  {
    "objectID": "posts/2021-11-21-Step-Detection.html",
    "href": "posts/2021-11-21-Step-Detection.html",
    "title": "Step Detection",
    "section": "",
    "text": "Detecting steps in noisy data is an important challenge in data analysis. Step detection falls under the scheme of change-point analysis methods, where, in this case, the change point corresponds to a transition in the level of the idealized signal (beyond changes in the signal due to noise and drift).\nHere, we try to detect steps in a time series dataset by modelling the shape of such steps in the data. For example, a dataset containing 2 steps has a different shape than one with 1 or no steps, and as such, this can be used to find the positions of these steps in a time series.\nHere, we generate templates which corresponds to steps at different time points, and compare these shapes to the observed data to find which template matches the shape of the data best. This allows us to identify the time points for the steps based on this matching. The only assumptions made in such an approach is that the steps are in the same direction (either all up or all down) and that the change in intensity associated with each step is identical.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2023-03-03-Colormaps.html",
    "href": "posts/2023-03-03-Colormaps.html",
    "title": "Accuracy of color representation using Bayesian shape calculations",
    "section": "",
    "text": "Introduction\nColor is a crucial element of images that plays a significant role in assigning meaning to information within the image, so, for that reason, the accurate color representation of data, plays an important role in science communication (Crameri, F. et al., 2020).\nHere we evaluate how well a colormap represents the underlying data by applying the Bayesian inference shape analysis framework in order to see if the colormap distorts the data.\n\n\nCode\nimport numpy as np\nnp.seterr(all='ignore')\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nimport matplotlib.colors as pltcol\nfrom scipy.special import betainc,betaln,gammaln\n\n\n\n\nCode\n# Change this section to change the colormaps\nc_maps = [plt.cm.jet, plt.cm.viridis, plt.cm.cividis, plt.cm.inferno, plt.cm.plasma, plt.cm.magma, plt.cm.rainbow]\nc_labels = [\"Jet\", \"Viridis\", \"Cividis\", \"Inferno\", \"Plasma\", \"Magma\", \"Rainbow\"]\ncolors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"gray\", \"black\"]\nmarkers = [\"o\", \"^\", \"s\", \"p\", \"*\", \"X\", \"D\"]\n\n\n\n\nCode\nlnprior_factor_location = 5.*np.log(10) +np.log(2.)## +10^5 to -10^5: 1/(\\delta \\mu)\nlnprior_factor_scale = np.log(2 * 3*np.log(10.)) ## +/-3 decades: 1/(\\delta ln(\\tau))\n\ndef ln_evidence(x,y):\n    ### 2.2.2\n    \n    N = float(x.size)\n    M = N/2.-1.\n    Ex = np.mean(x)\n    Ey = np.mean(y)\n    Exx = np.mean(x*x)\n    Eyy = np.mean(y*y)\n    Exy = np.mean(x*y)\n    vx = Exx-Ex*Ex\n    vy = Eyy-Ey*Ey\n    vxy = Exy-Ex*Ey\n\n    ## underflow protection\n    if vx <= 0 or vy <= 0 or vxy == 0: ## vxy can be negative\n        return -np.inf\n\n    r = vxy/np.sqrt(vx*vy)\n    r2 = r*r\n    out = gammaln(M) -N/2.*np.log(N) -.5*np.log(vx) -np.log(2) -2.*lnprior_factor_location -lnprior_factor_scale -M*np.log(np.pi) -M*np.log(vy) -M*np.log(1.-r2) + np.log(1.+r/np.abs(r)*betainc(.5,M,r2))\n\n    return out\n\n\n\n\nObtaining a simulated dataset\nHere we can see each colormap representation of simulating data (left) and its calculated lightness (right). For each calculation in the next section the lightness was used.\n\nTransforming data to lightness\nTo get the simulated lightness data for each color map we did the following:\n\nFirst let us simulate a 2D histogram picture, to replicate a more realistic data set, we use a random data generation followed by a median and gaussian filter.\nOnce we obtained the histogram dataset, we transformed each pixel to a color space coordinates, in this case, we used RGBA color space.\nCarry out the shape analysis between colormaps requires computing a most general parameter common to all colormaps, here we choose the relative lightness by using the luma relative lightness expression.\nThis analysis was carried out for by using a set of perceptually uniform sequential color maps implemented in matplotlib (Cividis, Viridis, Plasma, Inferno, and, Magma) and two of the miscellaneous colormaps (Jet and, Rainbow).\n\n\n\nCode\n#Normalized histogram Data\ndef his(n=10000, bins=30, seed=4):\n    np.random.seed(seed)\n    d = np.random.rand(n,2)\n    d_x_g = ndi.gaussian_filter(ndi.median_filter(d[:,0], size=10), sigma = 1)\n    d_y_g = ndi.gaussian_filter(ndi.median_filter(d[:,1], size=10), sigma = 1)\n    H, x_bin, y_bin = np.histogram2d(d_x_g, d_y_g, bins=(bins,bins))\n    f = (H-H.min())/(H.max()-H.min())\n    f = f.reshape((bins,bins,1))\n    return f\n\n\n\n\nCode\n#Transformation to RGBA\ndef to_rgba(f,cmap):\n    nx,ny,_ = f.shape\n    out = np.zeros([nx,ny,4])\n    for i in range(nx):\n        for j in range(ny):\n            out[i,j,:] = cmap(f[i,j,0])\n    return out\n\ndef compute_lightness(f,cmap):\n    c = to_rgba(f,cmap)\n    lum = 0.212 * c[:,:,0] + 0.715 * c[:,:,1] + 0.072 * c[:,:,2]\n    lum = lum[:,:,None]\n    return lum\n\n\n\n\nCode\nbins = 30\nf = his(bins=bins)\nlum_val = np.empty((len(c_maps), bins, bins,1))\nfor cmap_idx, cmap in enumerate (c_maps):\n    L = compute_lightness(f,cmap)\n    lum_val[cmap_idx] = L\n    \ncmap_L = plt.cm.gray\nfor cmap, L, label in zip(c_maps, lum_val, c_labels):\n    fig,ax = plt.subplots(1,2,figsize=(12,6))\n    ax[0].imshow(f, cmap=cmap,interpolation='nearest')\n    ax[0].set_title('Colormap Histogram')\n    ax[0].set_ylabel(label)\n    ax[1].imshow(L,cmap=cmap_L, interpolation='nearest', vmin=0, vmax=1)\n    ax[1].set_title('Calculated Ligthness')\n    ax[1].set_ylabel(label)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing the best colormap\nHere we compare the different colormaps to find which one best respresents the data. For each colormap, we used the shape analysis theory to compute the evidence that the original data is described by a template made from the lightness of that colormap (left). Then we used Bayesian model selection (BMS) to compute which is the best of these colormaps at representing the data (right).\n\n\nCode\nEvidence_val = []\nfor i in range(lum_val.shape[0]):\n    x = lum_val[i]\n    ev = ln_evidence(x,f)\n    Evidence_val.append(ev)\nlev = np.array(Evidence_val)\nlevd = lev - np.max(lev)\nprob = []\nfor i in range(levd.size):\n    p = 1./np.sum(np.exp(levd-levd[i]))\n    prob.append(p)\nprob = np.array(prob)\n\n\n\n\nCode\nfig, ax = plt.subplots(1,2,figsize=(14,5))\nbar_width = 0.5\nx_pro = np.arange(prob.size)\nx_evi = np.arange(lev.size)\n\nax[0].bar(x_evi, lev, width=bar_width, color=colors)\nax[0].set_xlabel('Colormaps')\nax[0].set_ylabel('ln (Evidence)')\nax[0].set_xticks(x_evi)\nax[0].set_xticklabels(c_labels)\nax[1].bar(x_pro, prob, width=bar_width, color=colors)\nax[1].set_xlabel('Colormaps')\nax[1].set_ylabel('Probability')\nax[1].set_xticks(x_pro)\nax[1].set_xticklabels(c_labels)\nplt.show()\n\n\n\n\n\nHere we show the calculated natural logarithm of evidence (left picture) where the higher values were obtained for the perceptually uniform sequential colormaps (i.e., cividis, viridis, inferno, plasma, magma). Although this group shares the same development principle, we obtained significantly different evidence values for all of them, where “Cividis” significantly better represents the data. Related to the other colormaps tested, “Jet” colormap is the one with the lowest evidence value, which means this one is the colormap with the least accurate reproduction of the simulated dataset.\n\n\nWhy is cividis better ??\nIn the figure below we show the lightness linearity of each colormap. The linearity directly correlates to the evidence values. This is because our Bayesian inference calculation directly compared the linearity of lightness to the data. In the plot below you can see Cividis is the colormap which has the most perceptually linear lightness, which is why it was the best of the colormaps we investigated.\n\n\nCode\nf_flat = f.reshape(900,1)\nn, h, w, c = lum_val.shape  \nlum_flat = lum_val.reshape(n, h*w*c)\n\nfig, ax = plt.subplots(figsize=(12,6))\nfor i in range(n):\n    ax.scatter(f_flat, lum_flat[i], color=colors[i%len(colors)], label=c_labels[i%len(c_labels)], marker = markers[i%len(markers)])\nax.legend()\nax.set_xlabel('Data Coordinates')\nax.set_ylabel('Ligthness')\nplt.show()"
  },
  {
    "objectID": "posts/2021-12-01-Peak-Search.html",
    "href": "posts/2021-12-01-Peak-Search.html",
    "title": "Template searching using BITS",
    "section": "",
    "text": "Introduction\nIt is often not clear how one can identify features of different scales in a signal, especially when the signal is distorted by noise. Here, we show that this can be achieved very simply by comparing the local shape of a signal to the previously known ideal shape of the feature, represented here by a template. The process we outline here, where the Bayes factor for the presence of the shape determined by a localised template (evidence_shape) vs the absence of any shape (evidence_flat) is determined at each point of the data, is a simplified proof of principle example for a generalised algorithm which we call Bayesian Inference-based Template Search (BITS), as described here.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport scipy.special\nfrom math import lgamma\n\n\ndef evidence_shape(y, x): # corresponds to Eq. 2.2.1 in SI for shape calculations\n    \n    N=float(x.size)\n\n    ex = np.mean(x)\n    exx = np.mean(x*x)\n    ey = np.mean(y)\n    eyy = np.mean(y*y)\n    exy = np.mean(x*y)\n    vx = exx - ex*ex\n    vy = eyy - ey*ey\n    vxy = exy - ex*ey\n    v2xy = vxy*vxy\n    r= vxy/np.sqrt(vx*vy)\n    r2 = r*r\n    m = (N-2.)/2.\n    \n    log_l = 0\n    log_delm = np.log(1e20) #this is the prior term for the scale\n    log_l += -log_delm\n    \n     \n    log_l += -m*np.log(np.pi)\n    log_l += -N/2*np.log(N)\n    log_l += lgamma(m) \n    log_l += -0.5*np.log(vx)\n    log_l += -m*np.log(vy)\n    log_l += -m*np.log(1.-r2) \n    log_l += -np.log(2)\n    log_l += np.log(1.+(r/np.abs(r))*sc.special.betainc(0.5,m,r2))\n        \n\n    return log_l\n\nprint(evidence_shape(np.random.rand(4), np.random.rand(4))) #checking it works\n\n-46.613923638849904\n\n\n\ndef evidence_flat(y): # corresponds to Eq. 2.2.4 in SI for shape calculations\n\n    N=float(y.size)\n\n    ey = np.mean(y)\n    eyy = np.mean(y*y)\n    vy = eyy - ey*ey\n    m = (N-2.)/2.\n\n    log_l = -(m + .5)*np.log(np.pi)\n    log_l += -(N/2)*np.log(N)\n    log_l +=  lgamma(m +.5) \n    log_l += -(m + 0.5)*np.log(vy)\n\n    return log_l\n\nprint(evidence_flat(np.random.rand(10)))\n\n-4.865221143158362\n\n\n\ndef make_peak(N, m, b):\n    x = np.arange(N)\n    y = m*np.exp(-((x - N/2)**2)/2/(N**1)) + b\n    \n    return y\n\nFirst, we compute the Bayes factor for a noisy signal with a Gaussian peak (red). The template (which is normalised to have height of 1) used is shown in black (mutliplied x100 for scale). The log_Bayes is printed. The Bayes factor shows that there is overwhelming evidence that the data is shaped like the template as opposed to being flat (i.e, just noise).\n\nwidth = 150\npeak = make_peak(width, 100.,0)*1 + np.random.normal(0., 10, width)\ntemplate = make_peak(width, 1., 0)\n\nplt.figure(1, figsize=(3,4))\nplt.xlim(0,150)\nplt.ylabel('Intensity')\nplt.plot(peak, 'r')\nplt.plot(template*100, 'k--',lw = 2)\nplt.show()\n\nprint('log_bayes =',evidence_shape(peak, template)- evidence_flat(peak))\n\n\n\n\nlog_bayes = 139.65598721558422\n\n\nNext we compare the same template(x100, in black) to a data set which we know contains no peaks, i.e., it is just noise (in red). Here, we see that the Bayes factor overwhelmingly shows that there is no peak, and that the variation in the data is only caused by noise.\n\nflat = np.random.normal(0., 10, width)\n\nplt.figure(1, figsize=(3,4))\nplt.xlim(0,150)\nplt.ylabel('Intensity')\nplt.plot(flat, 'r')\nplt.plot(template*100, 'k--',lw = 2)\nplt.show()\n\nprint('log_bayes =',evidence_shape(flat, template) - evidence_flat(flat))\n\n\n\n\nlog_bayes = -44.08800743753579\n\n\nWith the above results in mind, we next construct a dataset composed of multiple such Gaussian peaks. To get rid of the effect of scale, we randomise the scale for each of these peaks. The simulated dataset is plotted below.\n\nx = np.zeros(1000)\n\nwidth = 150\nstart = np.array([20, 176, 432,593,765])\n\nnp.random.seed(666)\n\nfor i in start:\n    x[i:i+width] = x[i:i+width] + (make_peak(width, np.abs(np.random.randn())*100,0))\n    \n    \nx = x + np.random.normal(0, 10, 1000)\n\nplt.figure(1, figsize=(10,4))\nplt.xlim(0,1000)\nplt.plot(x, 'r')\nplt.ylim(-35,135)\nplt.show()\n\n\n\n\nTo search for our template in this dataset, we make use of the fact that our template is supposed to be localised (i.e, it quickly drops to zero at distances far enough from the maximum). Therefore, we segment the full dataset into slices of the same size as the template, and compute the Bayes factor for the evidence of the shape vs the evidence of no shape for each of these slices. In effect, this is equivalent to performing a linear search of our localised template through the full dataset.\nThe log_Bayes for each location (after allowing for truncation at the edges) is plotted below (in blue). The locations of the centers of our simulated peaks, as previously known, are plotted as well (in black).\n\nbayes = []\n\ntemplate = make_peak(width, 1, 0)\nfor i in range(width//2, x.shape[0] - width//2):\n    temp_shape = x[i -width//2:i +width//2]\n    \n    bayes.append(-evidence_flat(temp_shape) + evidence_shape(temp_shape, template))\n\n    \nplt.figure(1, figsize=(10,4))\nplt.xlim(0,1000)\nplt.plot(x, 'r')\nplt.ylim(-35,135)\nplt.ylabel('Intensity')\nplt.show()\nplt.figure(2, figsize=(10,4))\nplt.xlim(0,1000)\nplt.ylim(-50,175)\nplt.plot(range(width//2, x.shape[0] - width//2),bayes, 'b')\nplt.plot(range(x.shape[0]),np.zeros_like(x), 'k', alpha = 0.3)\nplt.vlines(start + width//2, -55, 200, 'k', alpha = 0.3)\nplt.ylabel('log_Bayes')\nplt.show()\n\n\n\n\n\n\n\nThe log_Bayes factor is positive where there is more evidence for the presence of the shape, and negative where there is more evidence for its absence. We see that the Bayes factor is locally maximised at the previously known centers for our peaks, thereby showing that this approach can identify and localise features, in this case, Gaussian peaks, present in noisy data."
  },
  {
    "objectID": "posts/2022-09-08-Transition-BITS.html",
    "href": "posts/2022-09-08-Transition-BITS.html",
    "title": "Transition Detection Using BITS",
    "section": "",
    "text": "We have previously discussed using shapes for step/jump/transition detection: Step detection\nThis notebook, however, will serve as an addendum to that discussion, with more insight into Bayesian Inference-based Template Search (BITS) vs. general shape processing. We will be trying to find 2 transitions in datasets of variable length and will use both BITS and general shape comparison.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport scipy.special\nimport statistics as st\nfrom math import lgamma\nimport numba as nb\nfrom numba import jit\n\nLet’s start by generating some fake data. We will use the same setup as the previous notebook, and provide a sample dataset below.\n\n#generate data\n@nb.njit\ndef data_generator(num_points, num_jumps, jump_size, noise_size, drift_size, blur, seed):\n    \n    np.random.seed(444 + seed)\n\n    # parameters of fake data\n    nt = num_points\n    nj = num_jumps\n    \n    # generating noise & drift\n    tjumps = []\n    while len(tjumps) < nj:\n        time = int(np.random.exponential(scale=nt/2))\n        if time < nt-50 and time > 50:\n            if len(tjumps) > 0:\n                if np.min(np.abs(np.array(tjumps) - time)) > jump_size + blur:\n                    tjumps.append(time)\n            else:\n                tjumps.append(time)\n    tjumps.sort()\n    \n    zsteps = np.zeros(nj)\n    for i in range(nj):\n        zsteps[i] = jump_size\n    noise = np.ones(nt)\n    drift_arr = np.ones(nt)\n    for i in range(nt):\n        noise[i] = np.random.normal()*noise_size\n        drift_arr[i] = np.random.normal()*drift_size\n    drift = drift_arr.cumsum()\n    \n    # generating \"true\" path\n    z = np.zeros(nt)\n    \n    for i in range(nj):\n        z[tjumps[i]:] += zsteps[i]\n        \n    data = z + drift + noise\n    \n    # calculating snr\n    snr = []\n    for i in range(len(zsteps)):\n        if i == 0:\n            snr.append(zsteps[i]/np.std((noise+drift)[:tjumps[i+1]]))\n        elif i == len(zsteps)-1:\n            snr.append(zsteps[-1]/np.std((noise+drift)[tjumps[-2]:]))\n        else:\n            snr.append((zsteps[i]/np.std((noise+drift)[tjumps[i]:tjumps[i+1]])))\n    '''\n    #print('SNR = ' + str(st.mean(snr)))\n    #plotting\n    fig,ax = plt.subplots(1,1,figsize=(12,4))\n    ax.plot(data,'k',lw=.5);\n    #ax[1].plot(data,'o',color='k',markersize=1)\n    #ax[1].plot(z+drift,lw=2,color='r')\n    #for aa in ax:\n    #    for i in range(nj):\n    #        aa.axvline(x=tjumps[i],color='r',lw=1,zorder=-1,alpha=.5)\n    for i in range(nj):\n        ax.axvline(x=tjumps[i],color='r',lw=1,zorder=-1,alpha=.5)\n    ax.set_title('Actual Transitions')\n    #ax[1].set_title('Simulated Path')\n    plt.show()\n    '''  \n    return data, tjumps, np.mean(np.array(snr))\n\n\ndata, tjumps, snr = data_generator(num_points=1000, num_jumps=2, jump_size=80, noise_size=20,\n                                          drift_size=1, blur = 0, seed = 45)\nprint('SNR = %.3f'%snr)\n\n#plotting\nfig,ax = plt.subplots(1,1,figsize=(5,4))\nax.plot(data,'k',lw=.5);\nfor i in range(len(tjumps)):\n    ax.axvline(x=tjumps[i],color='r',lw=1,zorder=-1,alpha=.5)\n#ax.set_title('Actual Transitions')\nplt.xlim(0,1000)\nplt.savefig('/home/anjali/Documents/BITS_figures/Representative_trace_2.pdf')\nplt.show()\n\nSNR = 3.559\n\n\n\n\n\nWe now create an evidence function corresponding to equation 2.2.1 in the SI of the Bayesian shape calculation paper. This function takes in data, a template, and a variable prior for m (scaling nuisance parameter) and returns the evidence that the input data has the same shape as the template.\n\n@nb.jit(nb.float64(nb.float64[:],nb.float64,nb.float64[:]),nopython=True)\ndef evidence_template(data, prior_m, template): # corresponds to Eq. 2.2.1 in SI for shape calculations\n    \n    # constant evidence statistics\n    N = float(data.size)\n    m = (N-2.)/2.\n\n    ey = np.mean(data)\n    eyy = np.mean(data*data)\n    vy = eyy - ey*ey + 1e-300 ## helps with over/underflow\n    \n    # priors for m (scale), b (offset), tau (noise)   \n    log_l = 0\n    log_del_m = np.log(prior_m) \n    log_del_b = np.log(1e5)\n    log_del_tau = np.log(1e5)\n    log_l -= (log_del_m + log_del_b + log_del_tau)\n        \n    # template-dependent evidence statistics\n    ex = np.mean(template)\n    exx = np.mean(template*template)\n    exy = np.mean(template*data)\n    vx = exx - ex*ex + 1e-300 ## helps with over/underflow\n    vxy = exy - ex*ey + 1e-300 ## helps with over/underflow\n    r= vxy/np.sqrt(vx*vy)\n    r2 = r*r\n    \n    # evidence integral\n    log_l += -m*np.log(np.pi)\n    log_l += -N/2*np.log(N)\n    log_l += lgamma(m) \n    log_l += -0.5*np.log(vx)\n    log_l += -m*np.log(vy)\n    log_l += -m*np.log(1.-r2) \n    log_l += -np.log(2)\n    #log_l += np.log(1.+(r/np.abs(r))*sc.special.betainc(0.5,m,r2))   \n\n    return log_l\n\nWe also create an evidence function corresponding to equation 2.2.4 in the SI of the Bayesian shape calculation paper. This function takes in data and returns the evidence that the input data can be described by a “flat” template. Conceptually, this “flat” template can be thought of as a null case and alternative hypothesis; it represents the absence of shape, or latent structure, in the data.\nIn the Bayesian shape calculation paper, we describe the importance of this null template as it allows us to move beyond “Which template in out set is optimal?” and instead ask the question: “Does the data look more like any of our templates than like uncorrelated noise?” By having this alternative hypothesis, we are not forced to overfit and choose the “best” answer from our set of templates. Instead, if no particular template has an evidence which dominates over the null template, we conclude that the data does not look like any of our templates.\n\n@nb.jit(nb.float64(nb.float64[:]),nopython=True)\ndef evidence_flat(data): # corresponds to Eq. 2.2.4 in SI for shape calculations\n    \n    N = float(data.size)\n    m = (N-2.)/2.\n\n    ey = np.mean(data)\n    eyy = np.mean(data*data)\n    vy = eyy - ey*ey + 1e-300 ## helps with over/underflow\n\n    # evidence integral\n    log_l = -(m + .5)*np.log(np.pi)\n    log_l += -(N/2)*np.log(N)\n    log_l += lgamma(m +.5) \n    log_l += -(m + 0.5)*np.log(vy)\n    log_del_b = np.log(1e5)\n    log_del_tau = np.log(1e5)\n    log_l -= (log_del_b + log_del_tau)\n    \n    return log_l\n\nIn the previous notebook, the goal was to find two arbitrarily located signal jumps in a dataset of 600 data points. To do so, an exhaustive set of 360,000 templates was created, wherein each template was the full length of the dataset, with all possible combinations of the 2 jump locations represented. This sort of general shape analysis is admittedly not very scalable. Even for this small example of 2 jumps, the set of templates required grows as \\(\\mathcal{O}(N^2)\\) where \\(N\\) is the number of points in the dataset.\nMore generally, the set of templates required scales both with the number of data points, \\(N\\) and the number of features to find, \\(R\\), as \\(\\mathcal{O}(N^{R})\\). The number of computations required, however, grows as \\(\\mathcal{O}(N^{R+1})\\). We pick up an extra +1 in the exponent from generating and comparing all \\(N^R\\) templates to \\(N\\) data points.\nBelow we have the code for generating our set of full data templates and calculating their associated evidences. The bayes factor of the template evidences is compared to the bayes factor for the null evidence in a process known as Bayesian Model Selection (BMS). Once the templates which maximize this comparison are identified, we use them to return as the most probable locations for transitions.\n\n@nb.jit(nb.float64[:,:](nb.float64[:],nb.float64),nopython=True)\ndef full_shape_ev(data, prior_m):\n    nt = len(data)\n    m = prior_m\n    lnevs = np.zeros((nt,nt)) + np.nan\n    temp = np.zeros_like(data)\n    \n    for i in range(nt): ## sweep jump 1\n        for j in range(nt): ## sweep jump 2\n            ## make the template\n            temp *= 0\n            temp[i:] = 1\n            temp[j:] = 2\n            \n            ## calculate evidence          \n            lnevs[i,j] = evidence_template(data, m, temp)\n            \n    return lnevs\n\n\n@nb.jit(nb.int64[:](nb.float64[:],nb.float64),nopython=True)\ndef full_shape(data, prior_m):\n    \n    ## calculate evidence          \n    ln_evs = full_shape_ev(data, 10**8)\n    if (np.isfinite(ln_evs)).any() == False:\n        print('NaN Alert')\n        return np.array([0])\n    else:\n        _l = ln_evs.flatten()\n        _l -= np.max(_l)\n    \n    ln_prior = -np.log(_l.size) ## equal 1/N priors\n    joint = np.exp(_l+ln_prior)\n    \n    ## use BMS to find transitions\n    bms_probs = joint/joint.sum()\n    bms_array = np.empty_like(ln_evs)\n    for i in range(0, _l.size, ln_evs.shape[0]):\n        ind = int((i+ln_evs.shape[0]) / ln_evs.shape[0] - 1)\n        bms_array[ind,:] = bms_probs[i:i+ln_evs.shape[0]]\n        \n    transitions = []\n    for i in np.nonzero(bms_array == bms_array.max()):\n        transitions.append(i[0])\n    \n    return np.array(transitions)\n\nHere, instead of creating thousands of templates that span the entire dataset, we exploit the localized nature the features we are looking for. We can generate a template that is just large enough to capture the behavior we are searching for, and then scan it across our dataset. We perform the usual BMS calculation on a slice of our data corresponding to the template location, before translating over one datapoint and repeating the process. In effect, this is equivalent to executing a linear search of our localized template through the full dataset (with the caveat that points on the edge will be missed).\nThis approach greatly allievates the scaling headache faced by the more general shape calculation approach described above. Here, we only need one template regardless of \\(N\\), the size of our dataset. Additionally, in this example, there are two transitions which have the same shape. Since we can move our template, we don’t have to treat them like distinct features and we can still use just one template, regardless of the number of transitions we have. The number of calculations required for BITS grows as \\(\\mathcal{O}(CN)\\) where \\(C\\) is the number of \\(\\textbf{distinct}\\) features we are searching for.\n\n@nb.jit(nb.int64[:](nb.float64[:],nb.float64,nb.int64,nb.int64),nopython=True)\ndef BITS(data, prior_m, tail_size, blur_size=0):\n    \n    # jump template\n    \"\"\"\n    tmp_jump = np.ones(tmp_size,dtype=nb.float64)\n    for i in range(tmp_size):\n        if i >= tmp_size / 2.:\n            tmp_jump[i] = 1.\n        else:\n            tmp_jump[i] = 0.\n    \"\"\"\n            \n    # blurred template\n    tt = tail_size // 2\n    temp = np.zeros(blur_size + 2 * tt,dtype=nb.float64) \n    tmp_size = blur_size + 2 * tt\n    ind = np.arange(tt, tt+blur_size, 1)\n    temp[ind] = np.linspace(0., 1., len(ind))\n    temp[-tt:] = 1\n    \n    # calculate evidences\n    ev_jump = np.empty(data.shape[0] - tmp_size)\n    ev_flat = np.empty(data.shape[0] - tmp_size)\n    \n    for i in range(0, data.shape[0] - tmp_size):\n        sub = data[i:i+tmp_size]\n        #print(type(sub[0]))\n        ev_jump[i] = evidence_template(sub, prior_m, temp)\n        ev_flat[i] = evidence_flat(sub)\n    \n    # use BMS to calculate posteriors\n    #ev_line = np.ones_like(ev_step) * -np.inf\n    bayes_jump = ev_flat - ev_jump\n    bayes_flat = ev_jump - ev_flat\n    \n    prob_jump = 1. / (1. + np.exp(bayes_jump))\n    prob_flat = 1. / (1. + np.exp(bayes_flat)) \n    \n    # taking out repeats\n    jump = np.where(prob_jump>=0.85)[0]\n    \n    if len(jump) == 0:\n        return jump\n    elif len(jump) == 1:\n         transitions_jump = jump\n    else:\n        if np.min(np.diff(jump))<=tmp_size:\n            jump_split = np.split(jump, np.argwhere(np.diff(jump)>=tmp_size).flatten()+1)\n            ind = 0\n            transitions_jump = np.ones(len(jump_split),dtype=nb.int64)\n            for i in jump_split:\n                transitions_jump[ind] = i[np.argmax(prob_jump[i])]\n                ind += 1 \n      \n    return transitions_jump + round(tmp_size / 2)\n\nLet’s take a look at how both of these methods scale.\n\nimport datetime\n\nBITS_times = []\nfull_times = []\n\ndp = [300, 600, 900, 1200, 1500, 1800, 2100, 2400, 2700, 3000]\nfor i in dp:\n    print('%d Datapoints complete'%(i))\n\n    ## generate data\n    data, tjumps, snr = data_generator(num_points=i, num_jumps=2, jump_size=80, noise_size=20,\n                                          drift_size=1, blur = 0, seed = 76)\n\n    timers_bits = np.zeros(10)\n    timers_full = np.zeros(10)\n    for i in range(10):\n        #print('iteration %d'%(i))\n        ## time BITS\n        start = datetime.datetime.now()\n        tran_BITS = BITS(data, 10**8, 55, blur_size=0)\n        finish = datetime.datetime.now()\n        if (tjumps == tran_BITS).all():\n            timers_bits[i] = (finish-start).total_seconds()\n        else:\n            print('failed to find all transitions - BITS')\n        \n        ## time full shape analysis\n        start = datetime.datetime.now()\n        tran_full = full_shape(data, 10**8)\n        finish = datetime.datetime.now()\n        if (tjumps == tran_full).all():\n            timers_full[i] = (finish-start).total_seconds()\n        else:\n            print('failed to find all transitions - full')\n        \n    full_times.append(np.median(timers_full))\n    BITS_times.append(np.median(timers_bits))\n\n300 Datapoints complete\n600 Datapoints complete\n900 Datapoints complete\n1200 Datapoints complete\n1500 Datapoints complete\n1800 Datapoints complete\n2100 Datapoints complete\n2400 Datapoints complete\n2700 Datapoints complete\n3000 Datapoints complete\n\n\n\nplt.figure(figsize=(5,4))\nplt.plot(dp, BITS_times, 'o', label='BITS')\nplt.plot(dp, full_times, 'o', label='Full Dataset Template')\nplt.legend(loc=0)\n#plt.title('Linear Axes')\nplt.xlabel('N')\nplt.xlim(200,3100)\nplt.ylabel('Run Time (s)')\nplt.savefig('/home/anjali/Documents/BITS_figures/2Features_linear.pdf')\nplt.show()\n\n\n\n\nWe note some exponential behavior for the full dataset template curve, while the BITS times grow linearly. Let’s investigate further with a loglog plot.\n\nfrom scipy.optimize import curve_fit\n\n## log plotting\nplt.figure(figsize=(5,4))\nplt.plot(np.log(dp), np.log(BITS_times), 'o', label='BITS')\nplt.plot(np.log(dp), np.log(full_times), 'o', label='Full Dataset Template')\nplt.legend(loc=0)\n\n## curve-fitting\ndef linear(x, m, b):\n    return m*x + b\n\npopt, pcov = scipy.optimize.curve_fit(linear, np.log(dp), np.log(BITS_times))\nplt.plot(np.log(dp), linear(np.log(dp), popt[0], popt[1]), 'k--')\nprint('BITS fit: y = %.3f*m + %.3f'%(np.round(popt[0],3), np.round(popt[1],3)))\n\npopt, pcov = scipy.optimize.curve_fit(linear, np.log(dp), np.log(full_times))\nplt.plot(np.log(dp), linear(np.log(dp), popt[0], popt[1]), 'k--')\nprint('Full Dataset Template fit: y = %.3f*m + %.3f'%(np.round(popt[0],3), np.round(popt[1],3)))\n\n## finishing up plot\n#plt.title('Log-Log Plot');\nplt.xlabel('Log(N)')\nplt.ylabel('Log(Run Time) (s)')\nplt.savefig('/home/anjali/Documents/BITS_figures/2Features_loglog.pdf')\n\nBITS fit: y = 1.009*m + -14.301\nFull Dataset Template fit: y = 2.908*m + -18.446\n\n\n\n\n\nAfter fitting, we see that our Big-\\(\\mathcal{O}\\) predictions are empirically confirmed. The BITS slope is \\(\\approx 1\\) which matches \\(\\mathcal{O}(CN) = \\mathcal{O}(N)\\), while the full dataset slope is \\(\\approx 3\\) which matches \\(\\mathcal{O}(N^{R+1}) = \\mathcal{O}(N^{3})\\)."
  },
  {
    "objectID": "posts/2021-12-08-Super-Resolution.html",
    "href": "posts/2021-12-08-Super-Resolution.html",
    "title": "Super-resolution Microscopy using BITS",
    "section": "",
    "text": "Introduction\nSuper resolution microscopy is an imaging technique where the diffraction limit is circumvented by localizing the center of diffraction-limited fluorophore “spots”.\nHere, we localize these spots in an image by finding the pixels (and later, the locations inside a pixel) where the templates for these spots have maximum evidence of matching the local shape of the data.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom scipy.special import betainc,betaln,gammaln\ndef lnevidence(x,y):\n    N=float(x.size)\n\n    ex = np.nanmean(x)\n    exx = np.nanmean(x*x)\n    ey = np.nanmean(y)\n    eyy = np.nanmean(y*y)\n    exy = np.nanmean(x*y)\n    vx = exx - ex*ex + 1e-300\n    vy = eyy - ey*ey + 1e-300\n    vxy = exy - ex*ey + 1e-300\n    r = vxy/np.sqrt(vx*vy)\n    r2 = r*r\n    M = (N-2.)/2.\n\n    lndels = 3.*np.log(1e30)\n    lnev = gammaln(M) - N/2.*np.log(N) -.5*np.log(vx) - np.log(2.) - lndels - M*(np.log(np.pi)+np.log(vy)+np.log(1.-r2)) + np.log(1.+np.sign(r)*betainc(.5,M,r2))\n    return lnev\n\nFirst, let us simulate a fluorescence image of a single molecule. To replicate a realistic dataset, we add Poisson noise to our image.\n\nnp.random.seed(8)\n\nnxy = 17\nw = 1.25\nI0 = 80.\noffset = 200.\nbounds = 1.0\nextra_molecules = 4\n\nmu = (np.random.rand(2)-.5)*bounds\nxy = np.linspace(-(nxy-1.)//2,(nxy-1.)//2,nxy)\ngx,gy = np.meshgrid(xy,xy,indexing='ij')\n\nz = np.random.poisson(I0*np.exp(-.5/(w**2.)*((mu[0]-gx)**2.+(mu[1]-gy)**2.))) + 0.\nprint('Nphotons (main)',z.sum())\nprint('Centroid (main) %.3f %.3f'%(mu[1],mu[0]))\nz += offset\n\nseparate = np.zeros((extra_molecules+1,nxy,nxy))\nseparate[0] = z.copy()\n\nfor i in range(extra_molecules):\n    _mu = (np.random.rand(2)-.5)*nxy*1.1\n    _z = np.random.poisson(I0*np.exp(-.5/(w**2.)*((_mu[0]-gx)**2.+(_mu[1]-gy)**2.))) + 0.\n    z += _z\n    separate[i+1] = _z.copy()\n\nfig,ax=plt.subplots(1,figsize=(6,6))\nax.set_title('Composite')\nax.imshow(z,interpolation='nearest',cmap='Greys_r',origin='lower',extent=[xy.min(),xy.max(),xy.min(),xy.max()])\nax.axvline(mu[0],color='r',alpha=.8)\nax.axhline(mu[1],color='r',alpha=.8)\nplt.show()\n\n\nif extra_molecules > 0:\n    fig,ax = plt.subplots(1,separate.shape[0],figsize=(separate.shape[0]*2.,2))\n    fig.suptitle('Separate Molecules')\n    for i in range(separate.shape[0]):\n        ax[i].imshow(separate[i],interpolation='nearest',cmap='Greys_r',origin='lower',extent=[xy.min(),xy.max(),xy.min(),xy.max()])\n    plt.show()\n\n\nNphotons (main) 750.0\nCentroid (main) 0.469 0.373\n\n\n\n\n\n\n\n\n\n\nInitial localization\nFirst, we grid the image into coarsely spread out points where a molecule might be centered. We’ll generate a template with some chosen guess width, and using the wrong noise model (i.e. Gaussian noise instead of Poisson noise), calculate the evidence for each template. We will zoom in on whichever grid point gives us the best evidence for the next step.\n\nns = 20\nsxy = np.linspace(xy.min(),xy.max(),ns)\nout = np.zeros((ns,ns))\n\nww = 1.5\n\nfrom tqdm.notebook import trange\nfor i in trange(sxy.size):\n    for j in range(sxy.size):\n        mx = sxy[i]\n        my = sxy[j]\n        template = np.exp(-.5/(ww**2.)*((mx-gx)**2.+(my-gy)**2.))\n        out[i,j] = lnevidence(template,z)\n\n#### Only use the central spot in this examples. Often other spots win first sighting....\nomax = out[nxy//2-2:nxy//2+2+1,nxy//2-2:nxy//2+2+1].max()\nfor i in range(sxy.size):\n    for j in range(sxy.size):\n        if out[i,j] == omax:\n            mx = sxy[i]\n            my = sxy[j]\n            break\n\nprint('Truth %.3f %.3f'%(mu[1],mu[0]))\nprint('Found %.3f %.3f'%(my,mx))\nfig,ax = plt.subplots(1,2,figsize=(12,6))\nax[0].imshow(out,interpolation='nearest',cmap='viridis',origin='lower',extent=[sxy.min(),sxy.max(),sxy.min(),sxy.max()])\nax[0].set_title('ln(evidence)')\n\nq = np.exp(out-out.max())/np.nansum(np.exp(out-out.max()))\n# q[np.bitwise_not(np.isfinite(q))] = 0.\nax[1].imshow(q,interpolation='nearest',cmap='viridis',origin='lower',extent=[sxy.min(),sxy.max(),sxy.min(),sxy.max()])\nax[1].set_title('Probability')\nfor aa in ax:\n    aa.axhline(y=mu[0],color='r',alpha=.8)\n    aa.axvline(x=mu[1],color='r',alpha=.8)\n    aa.axhline(mx,color='tab:blue',alpha=.8)\n    aa.axvline(my,color='tab:blue',alpha=.8)\nfor aa in ax:\n    aa.set_xlabel('Pixels')\n    aa.set_ylabel('Pixels')\nplt.show()\n\n\n\n\nTruth 0.469 0.373\nFound 0.421 0.421\n\n\n\n\n\nWe can see that our method has identified multiple local maxima. As an aside, this grid scanning and local shape calculation is exactly how our method of Bayesian Inference-based Template Search (BITS) works (you can find another example for this here. Note that we are only picking the central peak in this example.\n\n\nZooming in\nNow let’s zoom into the pixel we found the molecule located in. We will use the same process performed above: sub-divide this region into a grid, create templates (PSF) centered at each grid point, calculate the evidence, and then compare to find the best fit.\n\nns = 50\nsx = np.linspace(mx-.5,mx+.5,ns)\nsy = np.linspace(my-.5,my+.5,ns)\nout = np.zeros((ns,ns))\n\nww = 1.5\n\nprint('Init  %.3f %.3f'%(my,mx))\nfor i in range(sx.size):\n    for j in range(sy.size):\n        mx = sx[i]\n        my = sy[j]\n        template = np.exp(-.5/(ww**2.)*((mx-gx)**2.+(my-gy)**2.))\n        out[i,j] = lnevidence(template,z)\n\nomax = out.max()\nfor i in range(sx.size):\n    for j in range(sy.size):\n        if out[i,j] == omax:\n            mx = sx[i]\n            my = sy[j]\n            break\n            \nprint('Truth %.3f %.3f'%(mu[1],mu[0]))\nprint('Found %.3f %.3f'%(my,mx))\nfig,ax = plt.subplots(1,2,figsize=(12,6))\nax[0].imshow(out,interpolation='nearest',cmap='viridis',origin='lower',extent=[sy.min(),sy.max(),sx.min(),sx.max()])\nq = np.exp(out-out.max())/np.nansum(np.exp(out-out.max()))\nax[1].imshow(q,interpolation='nearest',cmap='viridis',origin='lower',extent=[sy.min(),sy.max(),sx.min(),sx.max()])\nfor aa in ax:\n    aa.axhline(mu[0],color='r',alpha=.8)\n    aa.axvline(mu[1],color='r',alpha=.8)\n    aa.axhline(mx,color='tab:blue',alpha=.8)\n    aa.axvline(my,color='tab:blue',alpha=.8)\nax[0].set_title('ln(evidence)')\nax[1].set_title('Probability')\nfor aa in ax:\n    aa.set_xlabel('Pixels')\n    aa.set_ylabel('Pixels')\nplt.show()\n\nInit  0.421 0.421\nTruth 0.469 0.373\nFound 0.492 0.390\n\n\n\n\n\n\n\nFinal Localization\n\nprint('Sub-pixel localization error (%%) %.3f %.3f'%(100*np.abs((my-mu[1])/1.),100*np.abs((mx-mu[0])/1.)))\n\nfig,ax=plt.subplots(1,figsize=(6,6))\nax.set_title('Composite')\nax.imshow(z,interpolation='nearest',cmap='Greys_r',origin='lower',extent=[xy.min(),xy.max(),xy.min(),xy.max()])\nax.axhline(mu[0],color='r',alpha=.8)\nax.axvline(mu[1],color='r',alpha=.8)\nax.axhline(mx,color='tab:blue',alpha=.8)\nax.axvline(my,color='tab:blue',alpha=.8)\nplt.show()\n\nSub-pixel localization error (%) 2.394 1.701"
  },
  {
    "objectID": "posts/signal_processing/2021-12-01-Signal-Processing.html",
    "href": "posts/signal_processing/2021-12-01-Signal-Processing.html",
    "title": "Signal processing using shapes",
    "section": "",
    "text": "Introduction\nThe instrument response function (or impulse response function, IRF) represents the distortion to a signal being measured. It may be defined as the measured output to an impulse (or a delta function input). The distortion by the IRF may be represented by a convolution operation. The output is a convolved signal of the input and the IRF. As such, the output thus needs to be deconvolved using the IRF to get the ‘true’ signal, which is a non-trivial process.\nTypically, convolution is easier than deconvolution. We utilise this here to deconvolve fluorescence lifetime data, as measured by a time-correlated single photon counting (TCSPC) instrument. This technique (indirectly) probes the time evolution of the excited state population of the fluorophore sample, which follows an exponential decay. Thus, the shape of the true signal should be best descibed by an exponential distribution parametrised with the fluorescence lifetime of the fluorophore. However, this is not the case for the measured signal, which is best described by a shape corresponding to the convolution of the exponential decay with the IRF. Thus, we can use templates generated by convolving different decays with a (known or unknown) IRF function to determine which decay and/or IRF is the most likely given some experimental data, thus indirectly deconvolving our fluorescence lifetime data.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport scipy.special\nfrom math import lgamma\n\n\ndef evidence(y, x): # corresponds to Eq. 2.2.2 in SI for shape calculations\n    \n    N=float(x.size)\n    \n    ex = np.mean(x)\n    exx = np.mean(x*x)\n    ey = np.mean(y)\n    eyy = np.mean(y*y)\n    exy = np.mean(x*y)\n    vx = exx - ex*ex\n    vy = eyy - ey*ey\n    vxy = exy - ex*ey\n    r= vxy/np.sqrt(vx*vy)\n    r2 = r**2\n    m = (N-2.)/2.\n     \n    log_l = -m*np.log(np.pi)\n    log_l += -N/2*np.log(N)\n    log_l += lgamma(m) \n    log_l += -0.5*np.log(vx)\n    log_l += -m*np.log(vy)\n    log_l += -m*np.log(1.-r2) \n    log_l += -np.log(2)\n    log_l += np.log(1.+(r/np.abs(r))*sc.special.betainc(0.5,m,r2))\n    \n    return log_l\n\n\nprint(evidence(np.random.rand(4), np.random.rand(4))) #checking it works\n\n-1.4922321503610445\n\n\nThe data collected is for the fluorescence lifetime of quininine (lit value = 19 ns) in the absence (q00) and in the presence of quenchers (q01-5). The time interval between data points is ~55ps (exact value given below). Each file comes embedded with the experimentally determined IRF (plotted below).\n\ntime, irf_q00, decay_q00, _, _ = np.loadtxt('q00_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ntau = np.arange(0, 3000)*(0.1097394/2)\nfake_decay = np.exp(-tau/19.49)\nsignal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n\n\nplt.plot(time, irf_q00, 'k')\nplt.plot(time, decay_q00)\nplt.plot(time, 1000*signal/(signal.max()))\nplt.xlim(15,30)\n\n\nprint(evidence(decay_q00, signal))\n\n-16066.958041718837\n\n\n\n\n\n\n\nDeconvolution based on known IRF\nThe first approach is a brute force one. 4000 points are taken between 1 and 100 (ns), and the decay curves corresponding to these points are generated. These decays are convolved with the known IRF to generate templates which are then compared with the shape of the experimental decay using a log-evidence for the shape comparison. If we assume that the prior for the lifetime is uniform, the evidence is proportional to the posterior of the lifetime. This log-evidence is plotted for the 4000 points. The MAP is printed.\nNotes: 1) Numpy’s convolve function has three modes, ‘full’(default), ‘same’, and ‘overlap’. These determine the size of the convolved array (see docs for exact description). Ideally, since we need the same size for our template and data, one would assume that the ‘same’ mode is most suitable. However, I tried this and it seems ‘same’ truncates the convolved array in the wrong side (from the beginning and not the end). So I went with convolving in the ‘full’ mode, and then truncating manually.  2) There is a shift parameter in the fitting software, which for these datasets, turns up a value of 3 datapoints. For this comparison, no shift is implemented. If needed, one could marginalise or estimate it.\n\ny_00 = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n    y_00.append(evidence(decay_q00, signal))\n\ny_00 = np.array(y_00) \n\nplt.figure(1)\nplt.plot(ts, y_00, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_00)])\n\n\n\n\n19.492873218304574\n\n\n\ntime, irf_q01, decay_q01, _, _ = np.loadtxt('q01_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ny_01 = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q01, fake_decay)[:time.shape[0]]\n \n    y_01.append(evidence(decay_q01, signal))\n\n    \ny_01 = np.array(y_01) \nplt.figure(1)\nplt.plot(ts, y_01, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_01)])\n\n\n\n\n8.822955738934734\n\n\n\ntime, irf_q02, decay_q02, _, _ = np.loadtxt('q02_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ny_02 = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n    \n    y_02.append(evidence(decay_q02, signal))\n\ny_02 = np.array(y_02) \nplt.figure(1)\nplt.plot(ts, y_02, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_02)])\n\n\n\n\n5.010502625656414\n\n\n\ntime, irf_q03, decay_q03, _, _ = np.loadtxt('q03_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ny_03 = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n    \n    y_03.append(evidence(decay_q03, signal))\n\n    \ny_03 = np.array(y_03) \nplt.figure(1)\nplt.plot(ts, y_03, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_03)])\n\n\n\n\n7.288072018004501\n\n\n\ntime, irf_q04, decay_q04, _, _ = np.loadtxt('q04_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ny_04 = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n    \n    y_04.append(evidence(decay_q04, signal))\n    \ny_04 = np.array(y_04) \nplt.figure(1)\nplt.plot(ts, y_04, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_04)])\n\n\n\n\n4.441110277569392\n\n\n\ntime, irf_q05, decay_q05, _, _ = np.loadtxt('q05_fit.txt', skiprows = 58).T\n\ntime = time*(0.1097394/2)\n\n\ny_05 = []\ny_p = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n    \n    y_05.append(evidence(decay_q05, signal))\n    \ny_05 = np.array(y_05) \nplt.figure(1)\nplt.plot(ts, y_05, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_05)])\n\n\n\n\n3.376594148537134\n\n\nThe above seems to agree with both the known value for quinine and the fits. Now we see what happens when we try to introduce a different IRF to the computation. In this case, I have used a Gaussian IRF centered at 22.5ns and with a sigma of 0.1ns. The comparison with the experimental IRF is given below.\n\nfake_decay = np.exp(-tau/19)\nsignal = np.convolve(irf_q00, fake_decay)[:time.shape[0]]\n\nfake_irf = np.zeros_like(irf_q00)\nfake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(0.1)**2))\n\nplt.plot(time, irf_q00, 'k')\nplt.plot(time, fake_irf, 'r')\n#plt.plot(time, decay_q00)\n#plt.plot(time, signal/25)\nplt.xlim(15,30)\nplt.show()\n\nprint(time[np.argmax(irf_q00)])\n\n\n\n\n22.4417073\n\n\n\ny_00_f = []\ny_p = []\n\ntau = time.copy()\n\nts = np.linspace(1,100, 4000)\n#print()\n\nfor t in ts:\n    fake_decay = np.exp(-tau/t)\n    signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n    \n    y_00_f.append(evidence(decay_q00, signal))\n    \ny_00_f = np.array(y_00_f) \nplt.figure(1)\nplt.plot(ts, y_00, 'k')\nplt.plot(ts, y_00_f, 'r')\nplt.show()\n\nprint(ts[np.argmax(y_00)])\nprint(ts[np.argmax(y_00_f)])\n\n\n\n\n19.492873218304574\n20.087021755438858\n\n\nThe above curve shows that M_guess has a lower evidence (area under the curve) than M_exp. This suggest that our guess IRF with a sigma of 0.1ns is a worse model for the IRF than the experimentally determined function, as would be expected. Now we see if we can find a better model for our IRF than our rudimentary guess.\n\n\nIRF Calibration based on Known Decay\nNext up, we try to estimate what the most probable width of the IRF is based on a known value of decay for a signal. For example, the literature value for the fluorescence lifetime of quinine is 19ns. If we use this value to calculate a decay, we want to find what width of IRF gives a signal which is most similar in shape to the experimental signal. Below, the posterior for the sigma of the IRF is shown, the MAP is printed and the IRF corresponding to this MAP sigma is compared to the experimentally determined IRF.\n\np_irf = []\n\ntau = time.copy()\n\nws = np.linspace(0.005, 1, 500)\n\nfake_decay = np.exp(-tau/19.)\n\nfor w in ws:\n    fake_irf = np.zeros_like(time)\n    fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2))\n    signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n        \n    p_irf.append(evidence(decay_q00, signal))\n\np_irf = np.array(p_irf)\n\nplt.plot(ws, p_irf, 'r')\nplt.show()\n\nprint(ws[np.argmax(p_irf)])\n\nfake_irf = np.zeros_like(time)\nfake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2))\n\nplt.plot(time, irf_q00, 'k')\nplt.plot(time, fake_irf, 'r')\nplt.xlim(18,27)\nplt.show()\n\n\n\n\n0.39781563126252506\n\n\n\n\n\nWe see that the inferred IRF looks similar to the experimental IRF. Thus, we can deconvolve in both directions, from a known IRF to an unknown decay, and from a known decay to an unknown IRF. But what happens if both are unknown?\n\n\nBlind Deconvolution\nNext, we see if we can infer both the width and decay from a signal where both are not known. This process is called ‘blind deconvolution’. Below, we brute force the entire 2D log-posterior for both the sigma and the lifetime. The 1D marginalised posteriors for both are shown, the MAPs are printed and the corresponding IRF/decay signal is compared to the experimental signal.\n\np_blind = []\n\ntau = time.copy()\n\nws = np.linspace(0.005, 1, 500)\nts = np.linspace(1,100, 500)\n\nfor w in ws:\n    fake_irf = np.zeros_like(time)\n    fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2))\n    p_decay = []\n    for t in ts:\n        fake_decay = np.exp(-tau/t)\n        signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n        p_decay.append(evidence(decay_q00, signal))\n    \n    p_blind.append(p_decay)\n\np_blind = np.array(p_blind)\n\np_irf = p_blind.sum(1)\nplt.plot(ws, p_irf, 'r')\nplt.show()\n\nprint(ws[np.argmax(p_irf)])\n\nfake_irf = np.zeros_like(time)\nfake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2))\n\nplt.plot(time, irf_q00, 'k')\nplt.plot(time, fake_irf, 'r')\nplt.xlim(18,27)\nplt.show()\n\np_decay = p_blind.sum(0)\nplt.plot(ts, p_decay, 'r')\nplt.show()\n\nprint(ts[np.argmax(p_decay)])\n\nfake_decay = np.exp(-tau/ts[np.argmax(p_decay)])\nsignal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n\n\nplt.plot(time, decay_q00, 'k')\nplt.plot(time, 1000*signal/(signal.max()), 'r')\n#plt.xlim(18,27)\nplt.show()\n\n\n\n\n0.36591182364729463\n\n\n\n\n\n\n\n\n19.649298597194388\n\n\n\n\n\n\nfrom matplotlib import cm\n\nThe surface plot of the 2D posterior is plotted below.\n\nt_g, w_g = np.meshgrid(ts, ws)\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\", \"proj_type\" : \"ortho\"})\nax.view_init(30, 45)\nax.plot_surface(w_g, t_g, p_blind, cmap = cm.coolwarm, antialiased = 'True')\n\n<mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x1f37b7adbe0>\n\n\n\n\n\nThe same is done for the dataset “q01”.\n\np_blind_01 = []\n\ntau = time.copy()\n\nws = np.linspace(0.005, 1, 500)\nts = np.linspace(1,100, 500)\n\nfor w in ws:\n    fake_irf = np.zeros_like(time)\n    fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2))\n    p_decay_01 = []\n    for t in ts:\n        fake_decay = np.exp(-tau/t)\n        signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n        p_decay_01.append(evidence(decay_q01, signal))\n    \n    p_blind_01.append(p_decay_01)\n\np_blind_01 = np.array(p_blind_01)\n\np_irf_01 = p_blind_01.sum(1)\nplt.plot(ws, p_irf_01, 'r')\nplt.show()\n\nprint(ws[np.argmax(p_irf_01)])\n\nfake_irf = np.zeros_like(time)\nfake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf_01)])**2))\n\nplt.plot(time, irf_q01, 'k')\nplt.plot(time, fake_irf, 'r')\nplt.xlim(18,27)\nplt.show()\n\np_decay_01 = p_blind_01.sum(0)\nplt.plot(ts, p_decay_01, 'r')\nplt.show()\n\nprint(ts[np.argmax(p_decay_01)])\n\nfake_decay = np.exp(-tau/ts[np.argmax(p_decay_01)])\nsignal_01 = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n\n\nplt.plot(time, decay_q01, 'k')\nplt.plot(time, 1000*signal_01/(signal_01.max()), 'r')\n#plt.xlim(18,27)\nplt.show()\n\n\n\n\n0.2642184368737475\n\n\n\n\n\n\n\n\n8.935871743486974\n\n\n\n\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\", \"proj_type\" : \"ortho\"})\nax.view_init(25, 35)\nax.plot_surface(w_g, t_g, p_blind_01, cmap = cm.coolwarm, antialiased = 'True')\n\n<mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x1f37b60ca90>\n\n\n\n\n\n\n\nMCMC Sampling\nBrute force grid searching is inefficient and hard to visualize, when the posterior has more than 2 dimensions. Therefore, we move to an MCMC sampling approach. First we show that our above approach of blind deconvolution for each individual signal can be replicated with MCMC.\n\nimport emcee as mc\nimport corner as cor\nfrom tqdm.notebook import tqdm\n\nThe posterior is defined using the evidence function between the guess signal and the experimental signal as the likelihood, and using uniform priors for sigma and lifetime.\nsigma ~ Uniform(0., 1.)  Lifetime ~ Uniform(0.,100.)\n\ndef log_post(param, decay, time):\n    if param[0] <= 0. or param[1] <= 0.:\n        return -np.inf\n    \n    if param[0] > 100. or param[1] > 1.:\n        return -np.inf\n    \n    t1 = param[0]\n    sigma = param[1]\n    fake_irf = np.zeros_like(time)\n    fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2))\n    \n    fake_decay = np.exp(-tau/t1)\n    signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n    \n    log_l = evidence(decay, signal)\n    \n    return log_l\n    \n\nWe define the same sampling condition for all datasets. The MCMC algorithm will use 50 walkers, the same initialisations, and will sample for 5000 steps, after a burn-in of 500 steps. For each dataset, the likelihood for all of the walkers are plotted both during the burn-in and production run, to ensure that the walkers are properly equilibriated during production.\nAt the end, uncorrrelated samples are collected from the MCMC chains by only accepting samples at the interval of the maximum autocorrelation time of the chains. A corner plot of the histograms of these uncorrelated samples is generated.\n\nndim = 2\nnwalkers = 50\n\nnp.random.seed(666)\np0 = [np.array([19., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)]\n\n\nsampler0 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q00, time])\n\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler0.sample(p0,iterations = init), total = init)): pass\np1 = sampler0.chain[:,-1].copy()\n\nplt.plot(sampler0.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler0.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler0.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler0.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples0 = sampler0.get_chain(flat=True)[::int(sampler0.get_autocorr_time().max())]\nprint(samples0.shape)\ncor.corner(samples0)\nplt.show()\n\n(8334, 2)\n\n\n\n\n\n\nsampler = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q01, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler.sample(p0,iterations = init), total = init)): pass\np1 = sampler.chain[:,-1].copy()\n\nplt.plot(sampler.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\nsampler.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples = sampler.get_chain(flat=True)[::int(sampler.get_autocorr_time().max())]\nprint(samples.shape)\nfig = cor.corner(samples)\nplt.show()\n\n(8065, 2)\n\n\n\n\n\n\nsampler2 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q02, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler2.sample(p0,iterations = init), total = init)): pass\np1 = sampler2.chain[:,-1].copy()\n\nplt.plot(sampler2.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\nsampler2.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler2.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler2.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples2 = sampler2.get_chain(flat=True)[::int(sampler2.get_autocorr_time().max())]\nprint(samples2.shape)\nfig = cor.corner(samples2)\nplt.show()\n\n(7813, 2)\n\n\n\n\n\n\nsampler3 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q03, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler3.sample(p0,iterations = init), total = init)): pass\np1 = sampler3.chain[:,-1].copy()\n\nplt.plot(sampler3.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler3.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler3.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler3.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples3 = sampler3.get_chain(flat=True)[::int(sampler3.get_autocorr_time().max())]\nprint(samples3.shape)\nfig = cor.corner(samples3)\nplt.show()\n\n(8621, 2)\n\n\n\n\n\n\nsampler4 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q04, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler4.sample(p0,iterations = init), total = init)): pass\np1 = sampler4.chain[:,-1].copy()\n\nplt.plot(sampler4.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler4.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler4.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler4.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples4 = sampler4.get_chain(flat=True)[::int(sampler4.get_autocorr_time().max())]\nprint(samples4.shape)\nfig = cor.corner(samples4)\nplt.show()\n\n(8334, 2)\n\n\n\n\n\n\nsampler5 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q05, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler5.sample(p0,iterations = init), total = init)): pass\np1 = sampler5.chain[:,-1].copy()\n\nplt.plot(sampler5.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler5.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler5.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler5.lnprobability.T, alpha = 0.1)\nplt.show()\n\n\n\n\n\n\n\n(50, 2)\n\n\n\n\n\n\n\n\n\nsamples5 = sampler5.get_chain(flat=True)[::int(sampler5.get_autocorr_time().max())]\nprint(samples5.shape)\nfig = cor.corner(samples5)\nplt.show()\n\n(8334, 2)\n\n\n\n\n\nWe see above that the results from the MCMC match the ones of the grid search relatively well. However, interestingly, in both cases, the blind deconvolution of individual signals lead to a slightly different result for the IRF. Ideally, the IRF should be the same for all signals. Can we get this consensus IRF by analysing multiple signals at once?\n\n\nConsensus Blind Deconvolution\nWe define a new posterior function which is capable of analysing an arbitrary numbe of decay functions simultaneously. The exact dimensionality of the posterior is determined by the input decay array.\n\ndef log_post2(param, decay, time):\n    if len(decay) != param.shape[0] - 1:\n        return np.NaN\n    \n    if np.any(param <= 0.):\n        return -np.inf\n    \n    if np.any(param[:-1] > 100.) or param[-1] > 1.:\n        return -np.inf\n    \n    \n    sigma = param[-1]\n    fake_irf = np.zeros_like(time)\n    fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2))\n    \n    log_l = 0\n    \n    tau = time\n    for i,t in enumerate(param[:-1]):\n        fake_decay = np.exp(-tau/t)\n        signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]]\n        log_l += evidence(decay[i], signal)\n    \n    return log_l\n    \n\nFirst, we see if we can model two decays simultaneously. This is a 3-dimensional problem. The code is written so that once the decay list has been constructed, the dimensionality of the problem is set. However, the initialization array still needs to be amended in terms of dimensionality.\n\ndecay = [decay_q00, decay_q01]\nndim = len(decay) + 1\nnwalkers = 50\n\nnp.random.seed(666)\np0 = [np.array([15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)]\n\nprint(log_post2(p0[0], decay, time))\n\nsampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time])\n\ninit = 500\nfor i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass\np1 = sampler_all1.chain[:,-1].copy()\n\nplt.plot(sampler_all1.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler_all1.reset()\n\nprint(p1.shape)\ninit = 5000\nfor i, result in enumerate(tqdm(sampler_all1.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler_all1.lnprobability.T, alpha = 0.1)\nplt.show()\n\n-40315.19945316909\n\n\n\n\n\n\n\n\n(50, 3)\n\n\n\n\n\n\n\n\n\nsamples_all1 = sampler_all1.get_chain(flat=True)[::int(sampler_all1.get_autocorr_time().max())]\nprint(samples_all1.shape)\nfig = cor.corner(samples_all1)\nplt.show()\n\n(6098, 3)\n\n\n\n\n\nThe MAPs for the decay lifetimes seem to agree with the 2D estimates. As expected, the consensus IRF sigma is estimated to be somewhere between the 2D estimates. The evolution of the likelihoods along the change also indicates that this is not a difficult space for the walkers to explore.\nNext, we move on to the problem of consensus blind deconvolution for all 6 decays. As can be seen, this posterior space is difficult to traverse, probably due to the high dimensionality. As a result, we start an initial run of 60 walkers for 2000 steps. Then we use the best 30 positions from those walkers at the end of their run to initialize a new set of 30 walkers. These walkers then have a burn-in of 1000 steps and a production run of 15000 steps. This allows us to pull uncorrelated samples for the posterior.\n\ndecay = [decay_q00, decay_q01, decay_q02, decay_q03, decay_q04, decay_q05]\nndim = len(decay) + 1\nnwalkers = 60\n\nnp.random.seed(666)\np0 = [np.array([15.,15., 15.,15.,15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)]\n\nprint(log_post2(p0[0], decay, time))\n\nsampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time])\n\ninit = 2000\nfor i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass\np1 = sampler_all1.chain[:,-1].copy()\n\nplt.plot(sampler_all1.lnprobability.T, alpha = 0.1)\nplt.show()\n\nlx = sampler_all1.lnprobability[-1,:].argsort()\np1 = sampler_all1.chain[-1,lx][-int(nwalkers/2):]\n\nsampler_all2 = mc.EnsembleSampler(int(nwalkers/2), ndim, log_post2, args=[decay, time])\ninit = 1000\nfor i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass\np2 = sampler_all2.chain[:,-1].copy()\n\nplt.plot(sampler_all2.lnprobability.T, alpha = 0.1)\nplt.show()\n\nsampler_all2.reset()\n\ninit = 15000\nfor i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass\n\nplt.plot(sampler_all2.lnprobability.T, alpha = 0.1)\nplt.show()\n\n-128578.10642327752\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamples_all2 = sampler_all2.get_chain(flat=True)[::int(sampler_all2.get_autocorr_time().max())]\nprint(samples_all2.shape)\nfig = cor.corner(samples_all2, labels = [r'$\\tau_{q00}$', r'$\\tau_{q01}$', r'$\\tau_{q02}$',\n                                         r'$\\tau_{q03}$', r'$\\tau_{q04}$', r'$\\tau_{q05}$',\n                                        r'$\\sigma$'])\nplt.show()\n\n(5358, 7)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Shape Calculation Examples",
    "section": "",
    "text": "This example gallery contains proof-of-principle examples showcasing how calculations of the shape of data using Bayesian inference can be utilized in techniques throughout the physical and life sciences.\nThese implementations focus on “toy problems” that mimic real experimental systems. Their purpose is not to provide robust solutions, but rather to demonstrate the breadth and simplicity of the Bayesian shape calcuation method. While the code and concepts displayed here are currently preliminary, we hope to develop them into stand-alone solutions in the future. In the meantime, the code for these examples is freely available for use.\n\n \n\n\nMore Information\nIf these examples and/or the associated code are useful in your research project, please cite:\nRay, K. K., Verma, A. R., Gonzalez Jr, R. L., Kinz-Thompson, C. D. (2022). Inferring the shape of data: A probabilistic framework for analyzing experiments in the natural sciences. Proc. R. Soc. A. 478: 20220177. DOI: 10.1098/rspa.2022.0177\nView the preprint: arXiv:2109.12462\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\nAccuracy of color representation using Bayesian shape calculations\n\n\n\n\n\n\n\ncolormaps\n\n\nlightness\n\n\nvisualization\n\n\n\n\nThis notebook shows an example to evaluate the accurate representation of data sets by using several current available color maps.\n\n\n\n\n\n\nMar 3, 2023\n\n\nAndres Cifuentes-Lopez, Korak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nTransition Detection Using BITS\n\n\n\n\n\n\n\ntemplate-search\n\n\nsteps\n\n\njumps\n\n\n\n\nThe following notebook deals with identifying features, in this case, jump transitions, from a noisy signal containing multiple such jumps. We also investigate the computational advantages of using BITS over a full template approach.\n\n\n\n\n\n\nSep 8, 2022\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nAutomatic Aperture Filtering for Microscopy Using Shapes\n\n\n\n\n\n\n\nmicroscopy\n\n\nfilter\n\n\nFourier\n\n\n\n\nThis notebook is an example of how to remove high frequency noise in your image by infering the shape of the aperture (in Fourier space) of your microscope.\n\n\n\n\n\n\nDec 8, 2021\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nSuper-resolution Microscopy using BITS\n\n\n\n\n\n\n\nfluorescence\n\n\nSMLM\n\n\nmicroscopy\n\n\nsingle-molecule\n\n\n\n\nThis notebook is an example of how to super resolve spots in fluorescence microscopy data (a la Single-Molecule Localization Microscopy).\n\n\n\n\n\n\nDec 8, 2021\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nTemplate searching using BITS\n\n\n\n\n\n\n\ntemplate-search\n\n\n\n\nThe following notebook deals with identifying features, in this case, Gaussian peaks, from a noisy signal containing multiple such peaks.\n\n\n\n\n\n\nDec 1, 2021\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nSignal processing using shapes\n\n\n\n\n\n\n\ndeconvolution\n\n\ncalibration\n\n\nMCMC\n\n\n\n\nThe following notebook deals with the issue of deconvolving a signal, in this case, the fluorescence lifetime decay curve, from a (known or unknown) instrument response function. The data shown here has been collected from real experiments by CKT.\n\n\n\n\n\n\nDec 1, 2021\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\n\n\nStep Detection\n\n\n\n\n\n\n\nsteps\n\n\njumps\n\n\n\n\nThis notebook is an example of how to detect multiple steps of unknown size and location using shapes\n\n\n\n\n\n\nNov 21, 2021\n\n\nKorak Ray, Anjali Verma, Ruben Gonzalez and Colin Kinz-Thompson\n\n\n\n\n\n\nNo matching items"
  }
]