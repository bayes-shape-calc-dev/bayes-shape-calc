{
  
    
        "post0": {
            "title": "Super-resolution Microscopy",
            "content": "Super resolution microscopy is an imaging technique where the diffraction limit is circumvented by localizing the center of diffraction-limited fluorophore &quot;spots&quot;. We localize these spots in an image by finding the pixels (and later, the locations inside a pixel) where the templates for these spots have maximum evidence of matching the local shape of the data. . import numpy as np import matplotlib.pyplot as plt . from scipy.special import betainc,betaln,gammaln def lnevidence(x,y): N=float(x.size) ex = np.nanmean(x) exx = np.nanmean(x*x) ey = np.nanmean(y) eyy = np.nanmean(y*y) exy = np.nanmean(x*y) vx = exx - ex*ex + 1e-300 vy = eyy - ey*ey + 1e-300 vxy = exy - ex*ey + 1e-300 r = vxy/np.sqrt(vx*vy) r2 = r*r M = (N-2.)/2. lndels = 3.*np.log(1e30) lnev = gammaln(M) - N/2.*np.log(N) -.5*np.log(vx) - np.log(2.) - lndels - M*(np.log(np.pi)+np.log(vy)+np.log(1.-r2)) + np.log(1.+np.sign(r)*betainc(.5,M,r2)) return lnev . First, let us simulate a fluorescence image of a single molecule. To replicate a real-world setting, we add Poisson noise to our image. . np.random.seed(8) nxy = 17 w = 1.25 I0 = 80. offset = 200. bounds = 1.0 extra_molecules = 4 mu = (np.random.rand(2)-.5)*bounds xy = np.linspace(-(nxy-1.)//2,(nxy-1.)//2,nxy) gx,gy = np.meshgrid(xy,xy,indexing=&#39;ij&#39;) z = np.random.poisson(I0*np.exp(-.5/(w**2.)*((mu[0]-gx)**2.+(mu[1]-gy)**2.))) + 0. print(&#39;Nphotons (main)&#39;,z.sum()) print(&#39;Centroid (main) %.3f %.3f&#39;%(mu[1],mu[0])) z += offset separate = np.zeros((extra_molecules+1,nxy,nxy)) separate[0] = z.copy() for i in range(extra_molecules): _mu = (np.random.rand(2)-.5)*nxy*1.1 _z = np.random.poisson(I0*np.exp(-.5/(w**2.)*((_mu[0]-gx)**2.+(_mu[1]-gy)**2.))) + 0. z += _z separate[i+1] = _z.copy() fig,ax=plt.subplots(1,figsize=(6,6)) ax.set_title(&#39;Composite&#39;) ax.imshow(z,interpolation=&#39;nearest&#39;,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,extent=[xy.min(),xy.max(),xy.min(),xy.max()]) ax.axvline(mu[0],color=&#39;r&#39;,alpha=.8) ax.axhline(mu[1],color=&#39;r&#39;,alpha=.8) plt.show() if extra_molecules &gt; 0: fig,ax = plt.subplots(1,separate.shape[0],figsize=(separate.shape[0]*2.,2)) fig.suptitle(&#39;Separate Molecules&#39;) for i in range(separate.shape[0]): ax[i].imshow(separate[i],interpolation=&#39;nearest&#39;,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,extent=[xy.min(),xy.max(),xy.min(),xy.max()]) plt.show() . Nphotons (main) 750.0 Centroid (main) 0.469 0.373 . Initial localization . We now grid the image into coarsely spread out points where a molecule might be centered. We&#39;ll generate a template with some chosen guess width, and using the wrong noise model (i.e. Gaussian noise instead of Poisson noise), calculate the evidence for each template. We will zoom in on whichever grid point gives us the best evidence for the next step. . ns = 20 sxy = np.linspace(xy.min(),xy.max(),ns) out = np.zeros((ns,ns)) ww = 1.5 from tqdm.notebook import trange for i in trange(sxy.size): for j in range(sxy.size): mx = sxy[i] my = sxy[j] template = np.exp(-.5/(ww**2.)*((mx-gx)**2.+(my-gy)**2.)) out[i,j] = lnevidence(template,z) #### Only use the central spot in this examples. Often other spots win first sighting.... omax = out[nxy//2-2:nxy//2+2+1,nxy//2-2:nxy//2+2+1].max() for i in range(sxy.size): for j in range(sxy.size): if out[i,j] == omax: mx = sxy[i] my = sxy[j] break print(&#39;Truth %.3f %.3f&#39;%(mu[1],mu[0])) print(&#39;Found %.3f %.3f&#39;%(my,mx)) fig,ax = plt.subplots(1,2,figsize=(12,6)) ax[0].imshow(out,interpolation=&#39;nearest&#39;,cmap=&#39;viridis&#39;,origin=&#39;lower&#39;,extent=[sxy.min(),sxy.max(),sxy.min(),sxy.max()]) ax[0].set_title(&#39;ln(evidence)&#39;) q = np.exp(out-out.max())/np.nansum(np.exp(out-out.max())) # q[np.bitwise_not(np.isfinite(q))] = 0. ax[1].imshow(q,interpolation=&#39;nearest&#39;,cmap=&#39;viridis&#39;,origin=&#39;lower&#39;,extent=[sxy.min(),sxy.max(),sxy.min(),sxy.max()]) ax[1].set_title(&#39;Probability&#39;) for aa in ax: aa.axhline(y=mu[0],color=&#39;r&#39;,alpha=.8) aa.axvline(x=mu[1],color=&#39;r&#39;,alpha=.8) aa.axhline(mx,color=&#39;tab:blue&#39;,alpha=.8) aa.axvline(my,color=&#39;tab:blue&#39;,alpha=.8) for aa in ax: aa.set_xlabel(&#39;Pixels&#39;) aa.set_ylabel(&#39;Pixels&#39;) plt.show() . Truth 0.469 0.373 Found 0.421 0.421 . We can see that our method has identified multiple local maxima. As an aside, this grid scanning and local shape calculation is exactly how BITS works. Note that we are only picking the central peak in this example. . Zoom in . Now let&#39;s zoom into the pixel we found the molecule located in. We will use the same process performed above: sub-divide this region into a grid, create templates (PSF) centered at each grid point, calculate the evidence, and then compare to find the best fit. . ns = 50 sx = np.linspace(mx-.5,mx+.5,ns) sy = np.linspace(my-.5,my+.5,ns) out = np.zeros((ns,ns)) ww = 1.5 print(&#39;Init %.3f %.3f&#39;%(my,mx)) for i in range(sx.size): for j in range(sy.size): mx = sx[i] my = sy[j] template = np.exp(-.5/(ww**2.)*((mx-gx)**2.+(my-gy)**2.)) out[i,j] = lnevidence(template,z) omax = out.max() for i in range(sx.size): for j in range(sy.size): if out[i,j] == omax: mx = sx[i] my = sy[j] break print(&#39;Truth %.3f %.3f&#39;%(mu[1],mu[0])) print(&#39;Found %.3f %.3f&#39;%(my,mx)) fig,ax = plt.subplots(1,2,figsize=(12,6)) ax[0].imshow(out,interpolation=&#39;nearest&#39;,cmap=&#39;viridis&#39;,origin=&#39;lower&#39;,extent=[sy.min(),sy.max(),sx.min(),sx.max()]) q = np.exp(out-out.max())/np.nansum(np.exp(out-out.max())) ax[1].imshow(q,interpolation=&#39;nearest&#39;,cmap=&#39;viridis&#39;,origin=&#39;lower&#39;,extent=[sy.min(),sy.max(),sx.min(),sx.max()]) for aa in ax: aa.axhline(mu[0],color=&#39;r&#39;,alpha=.8) aa.axvline(mu[1],color=&#39;r&#39;,alpha=.8) aa.axhline(mx,color=&#39;tab:blue&#39;,alpha=.8) aa.axvline(my,color=&#39;tab:blue&#39;,alpha=.8) ax[0].set_title(&#39;ln(evidence)&#39;) ax[1].set_title(&#39;Probability&#39;) for aa in ax: aa.set_xlabel(&#39;Pixels&#39;) aa.set_ylabel(&#39;Pixels&#39;) plt.show() . Init 0.421 0.421 Truth 0.469 0.373 Found 0.492 0.390 . Final Localization . print(&#39;Sub-pixel localization error (%%) %.3f %.3f&#39;%(100*np.abs((my-mu[1])/1.),100*np.abs((mx-mu[0])/1.))) fig,ax=plt.subplots(1,figsize=(6,6)) ax.set_title(&#39;Composite&#39;) ax.imshow(z,interpolation=&#39;nearest&#39;,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,extent=[xy.min(),xy.max(),xy.min(),xy.max()]) ax.axhline(mu[0],color=&#39;r&#39;,alpha=.8) ax.axvline(mu[1],color=&#39;r&#39;,alpha=.8) ax.axhline(mx,color=&#39;tab:blue&#39;,alpha=.8) ax.axvline(my,color=&#39;tab:blue&#39;,alpha=.8) plt.show() . Sub-pixel localization error (%) 2.394 1.701 .",
            "url": "https://bayes-shape-calc.github.io/examples/fluorescence/smlm/microscopy/single%20molecules/2021/12/08/Super-Resolution.html",
            "relUrl": "/fluorescence/smlm/microscopy/single%20molecules/2021/12/08/Super-Resolution.html",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Automatic Aperture Filtering for Microscopy Using Shapes",
            "content": "Overview . The aperture/pupil in the back focal plane (BFP) of a microscope determines the resolution of the image that can be captured by the microscope. For a microscope, the aperture is basically the tube inside the microscope through which light passes. The BFP exists in Fourier space, and the aperture essentially blocks spatial frequencies beyond its width at the BFP. In the corresponding image at the camera, any spatial frequencies beyond the aperture width in the BFP cannot possibly have been made by the sample, because they were blocked by the aperture from getting to the camera. The goal is to utilize this understanding of the inner workings of the microscope to denoise a microscopy image by accounting for the aperture/pupil in the BFP. . We remove high-frequency noise that is beyond the aperture by modeling the pupil function of the image (i.e., the aperture) as a big circle and removing frequencies in the image outside of the aperture. By Fourier transforming the image, we expect to see non-zero power from spatial frequencies out to some radius away from the center -- that circle is the aperture. Beyond this circle, the power should be zero, based on our physical understanding of a microscope. . In this example, we vary the radius of the circle to model the Fourier transformed image as zeros outside of the circle, and some number greater than zero inside the circle. Once the best pupil function is found, i.e. the one whose shape best matches the experimentally observed Fourier image, any spatial frequencies in the image beyond this point are zeroed out to low-pass filter that noise out of the image. . We&#39;re using the Rat Hippocampal Neuron .tif file example from FIJI, which has several color channels in this notebook. . import numpy as np import matplotlib.pyplot as plt . from skimage import io img0 = io.imread(&#39;Rat_Hippocampal_Neuron.tif&#39;).astype(&#39;double&#39;) ## from FIJI examples ## Use the DIC image to find the aperture img = img0[-1] ## Plot image fig,ax=plt.subplots(1,1,figsize=(6,6)) ax.imshow(img,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std()) ax.set_title(&#39;Real Space&#39;) plt.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) plt.show() . ft = np.fft.fftshift(np.fft.fft2(img)) mag_img = np.sqrt(ft.real**2. + ft.imag**2.) phase_img = np.unwrap(np.arctan(ft.imag/ft.real)) ln_mag = np.log(mag_img) . fig,ax=plt.subplots(1,2,figsize=(12,6)) vmin,vmax = np.percentile(ln_mag,(1,99)) ax[0].imshow(ln_mag,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=vmin,vmax=vmax, extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2. ]) ax[0].set_title(&#39;Fourier Space (ln(Magnitude))&#39;) ax[1].imshow(phase_img,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;, extent=[-phase_img.shape[1]/2.,phase_img.shape[1]/2., -phase_img.shape[0]/2., phase_img.shape[0]/2. ]) ax[1].set_title(&#39;Fourier Space (Phase)&#39;) plt.show() . This evidence function calculates a circular mask/template given a radius, r0. Any pixels within r0 have value 1. and any outside r0 have value 0. It&#39;s like a 2D tophat function. . The evidence for this template is for m&gt;0, b in R, tau&gt;0 (SI 2.2.2). It is compared against the evidence for a flat or &#39;Null&#39; template (i.e., m=0, b in R, tau &gt;0 (SI 2.2.4). . This function returns the (negative) ratio of those two evidence functions so that the minimzer functions in scipy.optimize can find the maximum. . nx,ny = ft.shape kx,ky = np.mgrid[0:nx,0:ny] kx = kx.astype(&#39;double&#39;) - nx/2. ky = ky.astype(&#39;double&#39;) - ny/2. kr2 = kx**2. + ky**2. from scipy.special import betainc,betaln,gammaln def ln_bayes_factor(theta,y): r0 = theta ## model is out of bounds if r0 &lt; 5: return np.inf ## make the template x = (kr2 &lt; r0**2.).astype(&#39;double&#39;) N=float(x.size) ex = np.nanmean(x) exx = np.nanmean(x*x) ey = np.nanmean(y) eyy = np.nanmean(y*y) exy = np.nanmean(x*y) vx = exx - ex*ex + 1e-300 vy = eyy - ey*ey + 1e-300 vxy = exy - ex*ey + 1e-300 r = vxy/np.sqrt(vx*vy) r2 = r*r if r2 &lt; 1e-10 or r2 &gt; 1.-1e-10: return np.inf M = (N-2.)/2. delm = 1e30 lnR = np.log(2.) + np.log(delm) - betaln(.5,M) + .5*np.log(vx) - .5*np.log(vy) + M*np.log(1.-r2) - np.log(1.+np.sign(r)*betainc(.5,M,r2)) return lnR print(ln_bayes_factor((50.),ln_mag)) . -13934.824440777767 . xs = np.linspace(10,400,1000)*1. ev = np.zeros_like(xs) from tqdm.notebook import trange for i in trange(xs.size): ev[i] = -ln_bayes_factor((xs[i]),ln_mag) fig,ax = plt.subplots(1,figsize=(12,4)) ax.plot(xs,ev) plt.show() . from scipy.optimize import minimize def wrapper(initial_guess,ln_mag): return -1./(1.+np.exp(ln_bayes_factor(initial_guess,ln_mag))) initial_guess = np.array((xs[np.nanargmax(ev)])) out = minimize(wrapper, initial_guess, args=ln_mag, method=&#39;Nelder-Mead&#39;) print(out) r0 = out.x mask = (kr2 &lt; r0**2.).astype(&#39;int&#39;) . final_simplex: (array([[138.43843844], [138.43849125]]), array([-1., -1.])) fun: -1.0 message: &#39;Optimization terminated successfully.&#39; nfev: 53 nit: 18 status: 0 success: True x: array([138.43843844]) . fig,ax=plt.subplots(1,2,figsize=(12,6)) ax[0].imshow(ln_mag,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=vmin,vmax=vmax, extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2.]) ax[1].imshow(ln_mag*mask,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=vmin,vmax=vmax, extent=[-ln_mag.shape[1]/2.,ln_mag.shape[1]/2., -ln_mag.shape[0]/2., ln_mag.shape[0]/2.]) ax[0].set_title(&#39;Fourier Space&#39;) ax[1].set_title(&#39;Fourier Space (Masked)&#39;) plt.show() . filtered = ft*(mask+mask*1j) filtered = np.fft.ifft2(np.fft.fftshift(filtered)).real residual = img-filtered fig,ax=plt.subplots(3,2,figsize=(12,18)) ax[0,0].imshow(img,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std()) ax[0,1].imshow(filtered,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std()) ax[1,0].imshow(img,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std()) ax[1,1].imshow(filtered,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=img.mean()-1.*img.std(),vmax=img.mean()+1.*img.std()) ax[2,0].hist(residual.flatten(),bins=250,log=True) ax[2,1].imshow(residual,cmap=&#39;Greys_r&#39;,origin=&#39;lower&#39;,vmin=np.percentile(residual,35),vmax=np.percentile(residual,65)) for aa in ax[1]: aa.set_xlim(64,128) aa.set_ylim(64,128) ax[0,0].set_title(&#39;Real Space&#39;) ax[0,0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[0,1].set_title(&#39;Real Space (Filtered)&#39;) ax[0,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[1,0].set_title(&#39;Real Space (Zoom)&#39;) ax[1,0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[1,1].set_title(&#39;Real Space (Filtered, Zoom)&#39;) ax[1,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[2,0].set_title(&#39;Residual&#39;) ax[2,1].set_title(&#39;Residual (Accentuated)&#39;) ax[2,1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) plt.show() . filtered = np.zeros((nx,ny,3)) full = np.zeros_like(filtered) for i in range(3): ft = np.fft.fftshift(np.fft.fft2(img0[i])) fd = ft*(mask+mask*1j) fd = np.fft.ifft2(np.fft.fftshift(fd)).real filtered[:,:,i] = fd full[:,:,i] = img0[i] scaling = full.max((0,1))[None,None,:] filtered /= scaling full /= scaling fig,ax = plt.subplots(1,2,figsize=(16,8)) ax[0].imshow(full,origin=&#39;lower&#39;,interpolation=&#39;nearest&#39;) ax[0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[1].imshow(filtered,origin=&#39;lower&#39;,interpolation=&#39;nearest&#39;) ax[1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[0].set_title(&#39;Original&#39;) ax[1].set_title(&#39;Filtered&#39;) plt.show() fig,ax = plt.subplots(1,2,figsize=(16,8)) ax[0].imshow(full,origin=&#39;lower&#39;,interpolation=&#39;nearest&#39;) ax[1].imshow(filtered,origin=&#39;lower&#39;,interpolation=&#39;nearest&#39;) ax[0].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[1].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False) ax[0].set_title(&#39;Original (Zoom)&#39;) ax[1].set_title(&#39;Filtered (Zoom)&#39;) for aa in ax: aa.set_xlim(200,328) aa.set_ylim(200,328) plt.show() .",
            "url": "https://bayes-shape-calc.github.io/examples/filter/microscopy/fourier-transform/2021/12/08/Microscopy-Automatic-Aperture-Filtering.html",
            "relUrl": "/filter/microscopy/fourier-transform/2021/12/08/Microscopy-Automatic-Aperture-Filtering.html",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Signal processing using shapes",
            "content": "import numpy as np import matplotlib.pyplot as plt import scipy as sc import scipy.special from math import lgamma . def evidence(y, x): # corresponds to Eq. 2.2.2 in SI for shape calculations N=float(x.size) ex = np.mean(x) exx = np.mean(x*x) ey = np.mean(y) eyy = np.mean(y*y) exy = np.mean(x*y) vx = exx - ex*ex vy = eyy - ey*ey vxy = exy - ex*ey r= vxy/np.sqrt(vx*vy) r2 = r**2 m = (N-2.)/2. log_l = -m*np.log(np.pi) log_l += -N/2*np.log(N) log_l += lgamma(m) log_l += -0.5*np.log(vx) log_l += -m*np.log(vy) log_l += -m*np.log(1.-r2) log_l += -np.log(2) log_l += np.log(1.+(r/np.abs(r))*sc.special.betainc(0.5,m,r2)) return log_l print(evidence(np.random.rand(4), np.random.rand(4))) #checking it works . -1.4922321503610445 . The data collected is for the fluorescence lifetime of quininine (lit value = 19 ns) in the absence (q00) and in the presence of quenchers (q01-5). The time interval between data points is ~55ps (exact value given below). Each file comes embedded with the experimentally determined IRF (plotted below). . time, irf_q00, decay_q00, _, _ = np.loadtxt(&#39;q00_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . tau = np.arange(0, 3000)*(0.1097394/2) fake_decay = np.exp(-tau/19.49) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, decay_q00) plt.plot(time, 1000*signal/(signal.max())) plt.xlim(15,30) print(evidence(decay_q00, signal)) . -16066.958041718837 . Deconvolution based on known IRF . The first approach is a brute force one. 4000 points are taken between 1 and 100 (ns), and the decay curves corresponding to these points are generated. These decays are convolved with the known IRF to generate templates which are then compared with the shape of the experimental decay using a log-evidence for the shape comparison. If we assume that the prior for the lifetime is uniform, the evidence is proportional to the posterior of the lifetime. This log-evidence is plotted for the 4000 points. The MAP is printed. . Notes: 1) Numpy&#39;s convolve function has three modes, &#39;full&#39;(default), &#39;same&#39;, and &#39;overlap&#39;. These determine the size of the convolved array (see docs for exact description). Ideally, since we need the same size for our template and data, one would assume that the &#39;same&#39; mode is most suitable. However, I tried this and it seems &#39;same&#39; truncates the convolved array in the wrong side (from the beginning and not the end). So I went with convolving in the &#39;full&#39; mode, and then truncating manually. 2) There is a shift parameter in the fitting software, which for these datasets, turns up a value of 3 datapoints. For this comparison, no shift is implemented. If needed, one could marginalise or estimate it. . y_00 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_00.append(evidence(decay_q00, signal)) y_00 = np.array(y_00) plt.figure(1) plt.plot(ts, y_00, &#39;r&#39;) plt.show() print(ts[np.argmax(y_00)]) . 19.492873218304574 . time, irf_q01, decay_q01, _, _ = np.loadtxt(&#39;q01_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_01 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q01, fake_decay)[:time.shape[0]] y_01.append(evidence(decay_q01, signal)) y_01 = np.array(y_01) plt.figure(1) plt.plot(ts, y_01, &#39;r&#39;) plt.show() print(ts[np.argmax(y_01)]) . 8.822955738934734 . time, irf_q02, decay_q02, _, _ = np.loadtxt(&#39;q02_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_02 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_02.append(evidence(decay_q02, signal)) y_02 = np.array(y_02) plt.figure(1) plt.plot(ts, y_02, &#39;r&#39;) plt.show() print(ts[np.argmax(y_02)]) . 5.010502625656414 . time, irf_q03, decay_q03, _, _ = np.loadtxt(&#39;q03_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_03 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_03.append(evidence(decay_q03, signal)) y_03 = np.array(y_03) plt.figure(1) plt.plot(ts, y_03, &#39;r&#39;) plt.show() print(ts[np.argmax(y_03)]) . 7.288072018004501 . time, irf_q04, decay_q04, _, _ = np.loadtxt(&#39;q04_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_04 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_04.append(evidence(decay_q04, signal)) y_04 = np.array(y_04) plt.figure(1) plt.plot(ts, y_04, &#39;r&#39;) plt.show() print(ts[np.argmax(y_04)]) . 4.441110277569392 . time, irf_q05, decay_q05, _, _ = np.loadtxt(&#39;q05_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_05 = [] y_p = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_05.append(evidence(decay_q05, signal)) y_05 = np.array(y_05) plt.figure(1) plt.plot(ts, y_05, &#39;r&#39;) plt.show() print(ts[np.argmax(y_05)]) . 3.376594148537134 . The above seems to agree with both the known value for quinine and the fits. Now we see what happens when we try to introduce a different IRF to the computation. In this case, I have used a Gaussian IRF centered at 22.5ns and with a sigma of 0.1ns. The comparison with the experimental IRF is given below. . fake_decay = np.exp(-tau/19) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] fake_irf = np.zeros_like(irf_q00) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(0.1)**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) #plt.plot(time, decay_q00) #plt.plot(time, signal/25) plt.xlim(15,30) plt.show() print(time[np.argmax(irf_q00)]) . 22.4417073 . y_00_f = [] y_p = [] tau = time.copy() ts = np.linspace(1,100, 4000) #print() for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] y_00_f.append(evidence(decay_q00, signal)) y_00_f = np.array(y_00_f) plt.figure(1) plt.plot(ts, y_00, &#39;k&#39;) plt.plot(ts, y_00_f, &#39;r&#39;) plt.show() print(ts[np.argmax(y_00)]) print(ts[np.argmax(y_00_f)]) . 19.492873218304574 20.087021755438858 . The above curve shows that M_guess has a lower evidence (area under the curve) than M_exp. This suggest that our guess IRF with a sigma of 0.1ns is a worse model for the IRF than the experimentally determined function, as would be expected. Now we see if we can find a better model for our IRF than our rudimentary guess. . IRF Calibration based on Known Decay . Next up, we try to estimate what the most probable width of the IRF is based on a known value of decay for a signal. For example, the literature value for the fluorescence lifetime of quinine is 19ns. If we use this value to calculate a decay, we want to find what width of IRF gives a signal which is most similar in shape to the experimental signal. Below, the posterior for the sigma of the IRF is shown, the MAP is printed and the IRF corresponding to this MAP sigma is compared to the experimentally determined IRF. . p_irf = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) fake_decay = np.exp(-tau/19.) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_irf.append(evidence(decay_q00, signal)) p_irf = np.array(p_irf) plt.plot(ws, p_irf, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() . 0.39781563126252506 . We see that the inferred IRF looks similar to the experimental IRF. Thus, we can deconvolve in both directions, from a known IRF to an unknown decay, and from a known decay to an unknown IRF. But what happens if both are unknown? . Blind Deconvolution . Next, we see if we can infer both the width and decay from a signal where both are not known. This process is called &#39;blind deconvolution&#39;. Below, we brute force the entire 2D log-posterior for both the sigma and the lifetime. The 1D marginalised posteriors for both are shown, the MAPs are printed and the corresponding IRF/decay signal is compared to the experimental signal. . p_blind = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) ts = np.linspace(1,100, 500) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) p_decay = [] for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_decay.append(evidence(decay_q00, signal)) p_blind.append(p_decay) p_blind = np.array(p_blind) p_irf = p_blind.sum(1) plt.plot(ws, p_irf, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() p_decay = p_blind.sum(0) plt.plot(ts, p_decay, &#39;r&#39;) plt.show() print(ts[np.argmax(p_decay)]) fake_decay = np.exp(-tau/ts[np.argmax(p_decay)]) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] plt.plot(time, decay_q00, &#39;k&#39;) plt.plot(time, 1000*signal/(signal.max()), &#39;r&#39;) #plt.xlim(18,27) plt.show() . 0.36591182364729463 . 19.649298597194388 . from matplotlib import cm . The surface plot of the 2D posterior is plotted below. . t_g, w_g = np.meshgrid(ts, ws) fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;, &quot;proj_type&quot; : &quot;ortho&quot;}) ax.view_init(30, 45) ax.plot_surface(w_g, t_g, p_blind, cmap = cm.coolwarm, antialiased = &#39;True&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x1f37b7adbe0&gt; . The same is done for the dataset &quot;q01&quot;. . p_blind_01 = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) ts = np.linspace(1,100, 500) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) p_decay_01 = [] for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_decay_01.append(evidence(decay_q01, signal)) p_blind_01.append(p_decay_01) p_blind_01 = np.array(p_blind_01) p_irf_01 = p_blind_01.sum(1) plt.plot(ws, p_irf_01, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf_01)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf_01)])**2)) plt.plot(time, irf_q01, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() p_decay_01 = p_blind_01.sum(0) plt.plot(ts, p_decay_01, &#39;r&#39;) plt.show() print(ts[np.argmax(p_decay_01)]) fake_decay = np.exp(-tau/ts[np.argmax(p_decay_01)]) signal_01 = np.convolve(fake_irf, fake_decay)[:time.shape[0]] plt.plot(time, decay_q01, &#39;k&#39;) plt.plot(time, 1000*signal_01/(signal_01.max()), &#39;r&#39;) #plt.xlim(18,27) plt.show() . 0.2642184368737475 . 8.935871743486974 . fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;, &quot;proj_type&quot; : &quot;ortho&quot;}) ax.view_init(25, 35) ax.plot_surface(w_g, t_g, p_blind_01, cmap = cm.coolwarm, antialiased = &#39;True&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x1f37b60ca90&gt; . MCMC Sampling . Brute force grid searching is inefficient and hard to visualize, when the posterior has more than 2 dimensions. Therefore, we move to an MCMC sampling approach. First we show that our above approach of blind deconvolution for each individual signal can be replicated with MCMC. . import emcee as mc import corner as cor from tqdm.notebook import tqdm . The posterior is defined using the evidence function between the guess signal and the experimental signal as the likelihood, and using uniform priors for sigma and lifetime. . sigma ~ Uniform(0., 1.) Lifetime ~ Uniform(0.,100.) . def log_post(param, decay, time): if param[0] &lt;= 0. or param[1] &lt;= 0.: return -np.inf if param[0] &gt; 100. or param[1] &gt; 1.: return -np.inf t1 = param[0] sigma = param[1] fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2)) fake_decay = np.exp(-tau/t1) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] log_l = evidence(decay, signal) return log_l . We define the same sampling condition for all datasets. The MCMC algorithm will use 50 walkers, the same initialisations, and will sample for 5000 steps, after a burn-in of 500 steps. For each dataset, the likelihood for all of the walkers are plotted both during the burn-in and production run, to ensure that the walkers are properly equilibriated during production. . At the end, uncorrrelated samples are collected from the MCMC chains by only accepting samples at the interval of the maximum autocorrelation time of the chains. A corner plot of the histograms of these uncorrelated samples is generated. . ndim = 2 nwalkers = 50 np.random.seed(666) p0 = [np.array([19., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] . sampler0 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q00, time]) . init = 500 for i, result in enumerate(tqdm(sampler0.sample(p0,iterations = init), total = init)): pass p1 = sampler0.chain[:,-1].copy() plt.plot(sampler0.lnprobability.T, alpha = 0.1) plt.show() sampler0.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler0.sample(p1,iterations = init), total = init)): pass plt.plot(sampler0.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples0 = sampler0.get_chain(flat=True)[::int(sampler0.get_autocorr_time().max())] print(samples0.shape) cor.corner(samples0) plt.show() . (8334, 2) . sampler = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q01, time]) init = 500 for i, result in enumerate(tqdm(sampler.sample(p0,iterations = init), total = init)): pass p1 = sampler.chain[:,-1].copy() plt.plot(sampler.lnprobability.T, alpha = 0.1) plt.show() sampler.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler.sample(p1,iterations = init), total = init)): pass plt.plot(sampler.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples = sampler.get_chain(flat=True)[::int(sampler.get_autocorr_time().max())] print(samples.shape) fig = cor.corner(samples) plt.show() . (8065, 2) . sampler2 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q02, time]) init = 500 for i, result in enumerate(tqdm(sampler2.sample(p0,iterations = init), total = init)): pass p1 = sampler2.chain[:,-1].copy() plt.plot(sampler2.lnprobability.T, alpha = 0.1) plt.show() sampler2.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler2.sample(p1,iterations = init), total = init)): pass plt.plot(sampler2.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples2 = sampler2.get_chain(flat=True)[::int(sampler2.get_autocorr_time().max())] print(samples2.shape) fig = cor.corner(samples2) plt.show() . (7813, 2) . sampler3 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q03, time]) init = 500 for i, result in enumerate(tqdm(sampler3.sample(p0,iterations = init), total = init)): pass p1 = sampler3.chain[:,-1].copy() plt.plot(sampler3.lnprobability.T, alpha = 0.1) plt.show() sampler3.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler3.sample(p1,iterations = init), total = init)): pass plt.plot(sampler3.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples3 = sampler3.get_chain(flat=True)[::int(sampler3.get_autocorr_time().max())] print(samples3.shape) fig = cor.corner(samples3) plt.show() . (8621, 2) . sampler4 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q04, time]) init = 500 for i, result in enumerate(tqdm(sampler4.sample(p0,iterations = init), total = init)): pass p1 = sampler4.chain[:,-1].copy() plt.plot(sampler4.lnprobability.T, alpha = 0.1) plt.show() sampler4.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler4.sample(p1,iterations = init), total = init)): pass plt.plot(sampler4.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples4 = sampler4.get_chain(flat=True)[::int(sampler4.get_autocorr_time().max())] print(samples4.shape) fig = cor.corner(samples4) plt.show() . (8334, 2) . sampler5 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q05, time]) init = 500 for i, result in enumerate(tqdm(sampler5.sample(p0,iterations = init), total = init)): pass p1 = sampler5.chain[:,-1].copy() plt.plot(sampler5.lnprobability.T, alpha = 0.1) plt.show() sampler5.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler5.sample(p1,iterations = init), total = init)): pass plt.plot(sampler5.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples5 = sampler5.get_chain(flat=True)[::int(sampler5.get_autocorr_time().max())] print(samples5.shape) fig = cor.corner(samples5) plt.show() . (8334, 2) . We see above that the results from the MCMC match the ones of the grid search relatively well. However, interestingly, in both cases, the blind deconvolution of individual signals lead to a slightly different result for the IRF. Ideally, the IRF should be the same for all signals. Can we get this consensus IRF by analysing multiple signals at once? . Consensus Blind Deconvolution . We define a new posterior function which is capable of analysing an arbitrary numbe of decay functions simultaneously. The exact dimensionality of the posterior is determined by the input decay array. . def log_post2(param, decay, time): if len(decay) != param.shape[0] - 1: return np.NaN if np.any(param &lt;= 0.): return -np.inf if np.any(param[:-1] &gt; 100.) or param[-1] &gt; 1.: return -np.inf sigma = param[-1] fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2)) log_l = 0 tau = time for i,t in enumerate(param[:-1]): fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] log_l += evidence(decay[i], signal) return log_l . First, we see if we can model two decays simultaneously. This is a 3-dimensional problem. The code is written so that once the decay list has been constructed, the dimensionality of the problem is set. However, the initialization array still needs to be amended in terms of dimensionality. . decay = [decay_q00, decay_q01] ndim = len(decay) + 1 nwalkers = 50 np.random.seed(666) p0 = [np.array([15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] print(log_post2(p0[0], decay, time)) sampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time]) init = 500 for i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass p1 = sampler_all1.chain[:,-1].copy() plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() sampler_all1.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler_all1.sample(p1,iterations = init), total = init)): pass plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() . -40315.19945316909 . (50, 3) . samples_all1 = sampler_all1.get_chain(flat=True)[::int(sampler_all1.get_autocorr_time().max())] print(samples_all1.shape) fig = cor.corner(samples_all1) plt.show() . (6098, 3) . The MAPs for the decay lifetimes seem to agree with the 2D estimates. As expected, the consensus IRF sigma is estimated to be somewhere between the 2D estimates. The evolution of the likelihoods along the change also indicates that this is not a difficult space for the walkers to explore. . Next, we move on to the problem of consensus blind deconvolution for all 6 decays. As can be seen, this posterior space is difficult to traverse, probably due to the high dimensionality. As a result, we start an initial run of 60 walkers for 2000 steps. Then we use the best 30 positions from those walkers at the end of their run to initialize a new set of 30 walkers. These walkers then have a burn-in of 1000 steps and a production run of 15000 steps. This allows us to pull uncorrelated samples for the posterior. . decay = [decay_q00, decay_q01, decay_q02, decay_q03, decay_q04, decay_q05] ndim = len(decay) + 1 nwalkers = 60 np.random.seed(666) p0 = [np.array([15.,15., 15.,15.,15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] print(log_post2(p0[0], decay, time)) sampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time]) init = 2000 for i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass p1 = sampler_all1.chain[:,-1].copy() plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() lx = sampler_all1.lnprobability[-1,:].argsort() p1 = sampler_all1.chain[-1,lx][-int(nwalkers/2):] sampler_all2 = mc.EnsembleSampler(int(nwalkers/2), ndim, log_post2, args=[decay, time]) init = 1000 for i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass p2 = sampler_all2.chain[:,-1].copy() plt.plot(sampler_all2.lnprobability.T, alpha = 0.1) plt.show() sampler_all2.reset() init = 15000 for i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass plt.plot(sampler_all2.lnprobability.T, alpha = 0.1) plt.show() . -128578.10642327752 . samples_all2 = sampler_all2.get_chain(flat=True)[::int(sampler_all2.get_autocorr_time().max())] print(samples_all2.shape) fig = cor.corner(samples_all2, labels = [r&#39;$ tau_{q00}$&#39;, r&#39;$ tau_{q01}$&#39;, r&#39;$ tau_{q02}$&#39;, r&#39;$ tau_{q03}$&#39;, r&#39;$ tau_{q04}$&#39;, r&#39;$ tau_{q05}$&#39;, r&#39;$ sigma$&#39;]) plt.show() . (5358, 7) .",
            "url": "https://bayes-shape-calc.github.io/examples/deconvolution/calibration/mcmc/2021/12/01/Signal-Processing.html",
            "relUrl": "/deconvolution/calibration/mcmc/2021/12/01/Signal-Processing.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Template searching using BITS",
            "content": "It is often not clear how one can identify features of different scales in a signal, especially when the signal is distorted by noise. Here, we show that this can be achieved very simply by comparing the local shape of a signal to the previously known ideal shape of the feature, represented here by a template. The process we outline here, where the Bayes factor for the presence of the shape determined by a localised template (evidence_shape) vs the absence of any shape (evidence_flat) is determined at each point of the data, is a simplified proof of principle example for a generalised algorithm which we call Bayesian Inference-based Template Search (BITS), as described in [insert ref]. . import numpy as np import matplotlib.pyplot as plt import scipy as sc import scipy.special from math import lgamma . def evidence_shape(y, x): # corresponds to Eq. 2.2.1 in SI for shape calculations N=float(x.size) ex = np.mean(x) exx = np.mean(x*x) ey = np.mean(y) eyy = np.mean(y*y) exy = np.mean(x*y) vx = exx - ex*ex vy = eyy - ey*ey vxy = exy - ex*ey v2xy = vxy*vxy r= vxy/np.sqrt(vx*vy) r2 = r*r m = (N-2.)/2. log_l = 0 log_delm = np.log(1e20) #this is the prior term for the scale log_l += -log_delm log_l += -m*np.log(np.pi) log_l += -N/2*np.log(N) log_l += lgamma(m) log_l += -0.5*np.log(vx) log_l += -m*np.log(vy) log_l += -m*np.log(1.-r2) log_l += -np.log(2) log_l += np.log(1.+(r/np.abs(r))*sc.special.betainc(0.5,m,r2)) return log_l print(evidence_shape(np.random.rand(4), np.random.rand(4))) #checking it works . -46.613923638849904 . def evidence_flat(y): # corresponds to Eq. 2.2.4 in SI for shape calculations N=float(y.size) ey = np.mean(y) eyy = np.mean(y*y) vy = eyy - ey*ey m = (N-2.)/2. log_l = -(m + .5)*np.log(np.pi) log_l += -(N/2)*np.log(N) log_l += lgamma(m +.5) log_l += -(m + 0.5)*np.log(vy) return log_l print(evidence_flat(np.random.rand(10))) . -4.865221143158362 . def make_peak(N, m, b): x = np.arange(N) y = m*np.exp(-((x - N/2)**2)/2/(N**1)) + b return y . First, we compute the Bayes factor for a noisy signal with a Gaussian peak (red). The template (which is normalised to have height of 1) used is shown in black (mutliplied x100 for scale). The log_Bayes is printed. The Bayes factor shows that there is overwhelming evidence that the data is shaped like the template as opposed to being flat (i.e, just noise). . width = 150 peak = make_peak(width, 100.,0)*1 + np.random.normal(0., 10, width) template = make_peak(width, 1., 0) plt.figure(1, figsize=(3,4)) plt.xlim(0,150) plt.ylabel(&#39;Intensity&#39;) plt.plot(peak, &#39;r&#39;) plt.plot(template*100, &#39;k--&#39;,lw = 2) plt.show() print(&#39;log_bayes =&#39;,evidence_shape(peak, template)- evidence_flat(peak)) . log_bayes = 139.65598721558422 . Next we compare the same template(x100, in black) to a data set which we know contains no peaks, i.e., it is just noise (in red). Here, we see that the Bayes factor overwhelmingly shows that there is no peak, and that the variation in the data is only caused by noise. . flat = np.random.normal(0., 10, width) plt.figure(1, figsize=(3,4)) plt.xlim(0,150) plt.ylabel(&#39;Intensity&#39;) plt.plot(flat, &#39;r&#39;) plt.plot(template*100, &#39;k--&#39;,lw = 2) plt.show() print(&#39;log_bayes =&#39;,evidence_shape(flat, template) - evidence_flat(flat)) . log_bayes = -44.08800743753579 . With the above results in mind, we next construct a dataset composed of multiple such Gaussian peaks. To get rid of the effect of scale, we randomise the scale for each of these peaks. The simulated dataset is plotted below. . x = np.zeros(1000) width = 150 start = np.array([20, 176, 432,593,765]) np.random.seed(666) for i in start: x[i:i+width] = x[i:i+width] + (make_peak(width, np.abs(np.random.randn())*100,0)) x = x + np.random.normal(0, 10, 1000) plt.figure(1, figsize=(10,4)) plt.xlim(0,1000) plt.plot(x, &#39;r&#39;) plt.ylim(-35,135) plt.show() . To search for our template in this dataset, we make use of the fact that our template is supposed to be localised (i.e, it quickly drops to zero at distances far enough from the maximum). Therefore, we segment the full dataset into slices of the same size as the template, and compute the Bayes factor for the evidence of the shape vs the evidence of no shape for each of these slices. In effect, this is equivalent to performing a linear search of our localised template through the full dataset. . The log_Bayes for each location (after allowing for truncation at the edges) is plotted below (in blue). The locations of the centers of our simulated peaks, as previously known, are plotted as well (in black). . bayes = [] template = make_peak(width, 1, 0) for i in range(width//2, x.shape[0] - width//2): temp_shape = x[i -width//2:i +width//2] bayes.append(-evidence_flat(temp_shape) + evidence_shape(temp_shape, template)) plt.figure(1, figsize=(10,4)) plt.xlim(0,1000) plt.plot(x, &#39;r&#39;) plt.ylim(-35,135) plt.ylabel(&#39;Intensity&#39;) plt.show() plt.figure(2, figsize=(10,4)) plt.xlim(0,1000) plt.ylim(-50,175) plt.plot(range(width//2, x.shape[0] - width//2),bayes, &#39;b&#39;) plt.plot(range(x.shape[0]),np.zeros_like(x), &#39;k&#39;, alpha = 0.3) plt.vlines(start + width//2, -55, 200, &#39;k&#39;, alpha = 0.3) plt.ylabel(&#39;log_Bayes&#39;) plt.show() . The log_Bayes factor is positive where there is more evidence for the presence of the shape, and negative where there is more evidence for its absence. We see that the Bayes factor is locally maximised at the previously known centers for our peaks, thereby showing that this approach can identify and localise features, in this case, Gaussian peaks, present in noisy data. .",
            "url": "https://bayes-shape-calc.github.io/examples/template-search/2021/12/01/Peak-Search.html",
            "relUrl": "/template-search/2021/12/01/Peak-Search.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Step detection",
            "content": "import numpy as np import matplotlib.pyplot as plt . Simulate a Stepping System with Drift and Noise . Simulation of two jumps over 600 data points, each with SNR of ~3, which very reasonable for single-molecule data . np.random.seed(666) nt = 600 nj = 2 jump_size = 100. noise_size = 30. drift_size = 1. tjumps = np.random.randint(low=0,high=nt,size=nj) tjumps.sort() zsteps = np.random.normal(size=nj)*noise_size + jump_size noise = np.random.normal(size=nt)*noise_size drift = (np.random.normal(size=nt)*drift_size).cumsum() z = np.zeros(nt) for i in range(nj): z[tjumps[i]:] += zsteps[i] data = z + drift + noise snr1 = zsteps[0]/np.std((noise+drift)[:tjumps[1]]) snr2 = zsteps[1]/np.std((noise+drift)[tjumps[0]:]) . fig,ax = plt.subplots(1,2,figsize=(12,4)) ax[0].plot(data,&#39;k&#39;,lw=.5) ax[1].plot(data,&#39;o&#39;,color=&#39;k&#39;,markersize=1) ax[1].plot(z+drift,lw=2,color=&#39;r&#39;) for aa in ax: for i in range(nj): aa.axvline(x=tjumps[i],color=&#39;r&#39;,lw=1,zorder=-1,alpha=.5) ax[0].set_title(&#39;Data&#39;) ax[1].set_title(&#39;Simulated Path&#39;) plt.show() . Shape-based Evidence Function . Uses equation 2.2.1 in the SI of the shape-based calcuation paper. This is a custom loop (JITd in Numba for fast execution) that searches for all non-degenerate pairs of jumps (t1 &lt; t2) longer than 1 datapoint long. According to these templates and evidence, jumps can be both up or both down and still be detected. Having an up and then down jump (or vice versa) requires an additional template to be added to the loop . import numba as nb from scipy.special import betainc,gammaln from math import lgamma @nb.njit def shape_calculation(data): nt = data.size ## Constant evidence statistics N = float(nt) M = (N-2.)/2. ey = np.mean(data) eyy = np.mean(data*data) vy = eyy - ey*ey + 1e-300 ## helps with over/underflow lndelpriors = np.log(1e30) + np.log(1e30) + np.log(1e30) base_lnev = lgamma(M) -N/2.*np.log(N) - M*(np.log(np.pi) + np.log(vy)) - lndelpriors ## Initialize lnevs = np.zeros((nt,nt)) + np.nan template = np.zeros_like(data) for i in range(nt): ## sweep jump 1 for j in range(i+2,nt): ## sweep jump 2 ## make the template template *= 0 template[i:] = 1 template[j:] = 2 ## Template-dependent evidence statistics ex = np.mean(template) exx = np.mean(template*template) exy = np.mean(template*data) vx = exx - ex*ex + 1e-300 ## helps with over/underflow vxy = exy - ex*ey + 1e-300 ## helps with over/underflow r2= vxy**2./(vx*vy) ## Evidence - 2.2.1 - m in R, b in R, tau &gt; 0 ## allows for steps up and steps down lnevs[i,j] = base_lnev -.5*np.log(vx) lnevs[i,j] -= M*np.log(1.-r2) return lnevs . BMS Calculation to Find Jump Times . Calculate the evidences of each template (pair of jumps), then calculate the probability of them using Bayesian model selection (BMS) . ln_evs = shape_calculation(data) _l = ln_evs[np.isfinite(ln_evs)] _l -= np.max(_l) ln_prior = -np.log(_l.size) ## equal 1/N priors joint = np.exp(_l+ln_prior) bms_probs = np.zeros_like(ln_evs) bms_probs[np.isfinite(ln_evs)] = joint/joint.sum() # Jumps are found at location = np.nonzero(bms_probs == bms_probs.max()) t1 = location[0][0] t2 = location[1][0] . fig,ax = plt.subplots(1,2,figsize=(12,6)) for aa in ax: aa.imshow(ln_evs.T,origin =&#39;lower&#39;,interpolation=&#39;nearest&#39;) aa.set_xlabel(&#39;Time of jump 1&#39;) aa.set_ylabel(&#39;Time of jump 2&#39;) aa.set_title(&#39;ln(Evidence)&#39;) aa.axvline(tjumps[0],color=&#39;r&#39;,lw=.5) aa.axhline(tjumps[1],color=&#39;r&#39;,lw=.5) aa.axvline(t1,color=&#39;tab:blue&#39;,lw=.5) aa.axhline(t2,color=&#39;tab:blue&#39;,lw=.5) ax[1].set_xlim(tjumps[0]-10,tjumps[0]+11) ax[1].set_ylim(tjumps[1]-10,tjumps[1]+11) plt.show() . fig,ax = plt.subplots(1,2,figsize=(12,6)) for aa in ax: aa.imshow(bms_probs.T,origin =&#39;lower&#39;,interpolation=&#39;nearest&#39;) aa.set_xlabel(&#39;Time of jump 1&#39;) aa.set_ylabel(&#39;Time of jump 2&#39;) aa.set_title(&#39;Prob of jump times&#39;) aa.axvline(tjumps[0],color=&#39;r&#39;,lw=.5) aa.axhline(tjumps[1],color=&#39;r&#39;,lw=.5) aa.axvline(t1,color=&#39;tab:blue&#39;,lw=.5) aa.axhline(t2,color=&#39;tab:blue&#39;,lw=.5) ax[1].set_xlim(tjumps[0]-10,tjumps[0]+11) ax[1].set_ylim(tjumps[1]-10,tjumps[1]+11) plt.show() plt.show() . It&#39;s an exact match! Let&#39;s see where we&#39;ve predicted the jumps . from scipy import ndimage as nd idealized = np.zeros_like(data) width = 20 idealized[:t1] = nd.gaussian_filter(data[:t1],width) idealized[t1:t2] = nd.gaussian_filter(data[t1:t2],width) idealized[t2:] = nd.gaussian_filter(data[t2:],width) fig,ax = plt.subplots(1,1,figsize=(12,4)) ax.plot(data,&#39;ok&#39;,markersize=.5,lw=.5,label=&#39;Data&#39;) ax.plot(z+drift,lw=1,label=&#39;True path&#39;,color=&#39;red&#39;) ax.plot(idealized,lw=2,label=&#39;Idealized&#39;,color=&#39;tab:blue&#39;) ax.legend(loc=2) ax.axvline(x=tjumps[0],color=&#39;red&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=tjumps[1],color=&#39;red&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=t1,color=&#39;tab:blue&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=t2,color=&#39;tab:blue&#39;,lw=1,zorder=-1,alpha=.5) ax.set_title(&#39;Data&#39;) plt.show() . So it seems to work very well for low SNR jumps! . print(&#39;Found jump 1 (t=%d) at %d&#39;%(tjumps[0],t1)) print(&#39;Found jump 2 (t=%d) at %d&#39;%(tjumps[1],t2)) print(&#39;SNR for jump 1 = %f&#39;%(snr1)) print(&#39;SNR for jump 2 = %f&#39;%(snr2)) print(&#39;Probability of this model: %.3f&#39;%(np.sort(bms_probs.flatten())[-1:][0])) . Found jump 1 (t=236) at 236 Found jump 2 (t=429) at 429 SNR for jump 1 = 2.918031 SNR for jump 2 = 3.036615 Probability of this model: 0.681 . Super Hard Case (SNR &lt; 1) . note: we remove the drift noise because it&#39;s difficult to ensure the transitions have similar SNR otherwise... . np.random.seed(661) nt = 600 nj = 2 jump_size = 400. noise_size = 500. drift_size = 0. tjumps = np.random.randint(low=0,high=nt,size=nj) tjumps.sort() np.random.rand(666) ## keep randomized jumps but switch noise up zsteps = np.random.normal(size=nj)*noise_size/4 + jump_size noise = np.random.normal(size=nt)*noise_size drift = (np.random.normal(size=nt)*drift_size).cumsum() z = np.zeros(nt) for i in range(nj): z[tjumps[i]:] += zsteps[i] data = z + drift + noise snr1 = (zsteps[0])/np.std((noise+drift)[:tjumps[1]]) snr2 = (zsteps[1])/np.std((noise+drift)[tjumps[0]:]) fig,ax = plt.subplots(1,2,figsize=(12,4)) ax[0].plot(data,&#39;k&#39;,lw=.5) ax[1].plot(data,&#39;o&#39;,color=&#39;k&#39;,markersize=1) ax[1].plot(z+drift,lw=3,color=&#39;r&#39;) for aa in ax: for i in range(nj): aa.axvline(x=tjumps[i],color=&#39;r&#39;,lw=1,zorder=-1,alpha=.5) ax[0].set_title(&#39;Data&#39;) ax[1].set_title(&#39;Simulated Path&#39;) plt.show() ln_evs = shape_calculation(data) _l = ln_evs[np.isfinite(ln_evs)] _l -= np.max(_l) ln_prior = -np.log(_l.size) ## equal 1/N priors joint = np.exp(_l+ln_prior) bms_probs = np.zeros_like(ln_evs) bms_probs[np.isfinite(ln_evs)] = joint/joint.sum() # Jumps are found at location = np.nonzero(bms_probs == bms_probs.max()) t1 = location[0][0] t2 = location[1][0] # Plots fig,ax = plt.subplots(1,2,figsize=(12,6)) for aa in ax: aa.imshow(ln_evs.T,origin =&#39;lower&#39;,interpolation=&#39;nearest&#39;) aa.set_xlabel(&#39;Time of jump 1&#39;) aa.set_ylabel(&#39;Time of jump 2&#39;) aa.set_title(&#39;ln(Evidence)&#39;) aa.axvline(tjumps[0],color=&#39;r&#39;,lw=.5) aa.axhline(tjumps[1],color=&#39;r&#39;,lw=.5) aa.axvline(t1,color=&#39;tab:blue&#39;,lw=.5) aa.axhline(t2,color=&#39;tab:blue&#39;,lw=.5) ax[1].set_xlim(tjumps[0]-10,tjumps[0]+11) ax[1].set_ylim(tjumps[1]-10,tjumps[1]+11) plt.show() fig,ax = plt.subplots(1,2,figsize=(12,6)) for aa in ax: aa.imshow(bms_probs.T,origin =&#39;lower&#39;,interpolation=&#39;nearest&#39;) aa.set_xlabel(&#39;Time of jump 1&#39;) aa.set_ylabel(&#39;Time of jump 2&#39;) aa.set_title(&#39;Prob of jump times&#39;) aa.axvline(tjumps[0],color=&#39;r&#39;,lw=.5) aa.axhline(tjumps[1],color=&#39;r&#39;,lw=.5) aa.axvline(t1,color=&#39;tab:blue&#39;,lw=.5) aa.axhline(t2,color=&#39;tab:blue&#39;,lw=.5) ax[1].set_xlim(tjumps[0]-10,tjumps[0]+11) ax[1].set_ylim(tjumps[1]-10,tjumps[1]+11) plt.show() plt.show() # We can idealize the trace as the mean of each region idealized = np.zeros_like(data) idealized[:t1] = np.mean(data[:t1]) idealized[t1:t2] = np.mean(data[t1:t2]) idealized[t2:] = np.mean(data[t2:]) fig,ax = plt.subplots(1,1,figsize=(12,4)) ax.plot(data,&#39;ok&#39;,markersize=.5,lw=.5,label=&#39;Data&#39;) ax.plot(z+drift,lw=1,label=&#39;True path&#39;,color=&#39;red&#39;) ax.plot(idealized,lw=2,label=&#39;Idealized&#39;,color=&#39;tab:blue&#39;) ax.legend(loc=2) ax.axvline(x=tjumps[0],color=&#39;red&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=tjumps[1],color=&#39;red&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=t1,color=&#39;tab:blue&#39;,lw=1,zorder=-1,alpha=.5) ax.axvline(x=t2,color=&#39;tab:blue&#39;,lw=1,zorder=-1,alpha=.5) ax.set_title(&#39;Data&#39;) plt.show() print(&#39;Found jump 1 (t=%d) at %d&#39;%(tjumps[0],t1)) print(&#39;Found jump 2 (t=%d) at %d&#39;%(tjumps[1],t2)) print(&#39;SNR for jump 1 = %f&#39;%(snr1)) print(&#39;SNR for jump 2 = %f&#39;%(snr2)) . Found jump 1 (t=79) at 76 Found jump 2 (t=476) at 481 SNR for jump 1 = 0.569801 SNR for jump 2 = 0.894949 . pretty close!! But note that this model is not very likely! It only has a probability of: . print(&#39;Probability of this model: %.3f&#39;%(np.sort(bms_probs.flatten())[-1:][0])) . Probability of this model: 0.015 . This is partially an indication that it is very difficult to localize both jumps in this super noisy dataset. It is worth noting that it is much easier to find either of the jumps separately. We see this by marginalizing out the other jump and finding the maximum probability jump . print(&#39;Highest probability localization of just jump 1: %.3f&#39;%(np.sort(bms_probs.sum(1))[-1:][0])) print(&#39;Highest probability localization of just jump 2: %.3f&#39;%(np.sort(bms_probs.sum(0))[-1:][0])) . Highest probability localization of just jump 1: 0.114 Highest probability localization of just jump 2: 0.130 .",
            "url": "https://bayes-shape-calc.github.io/examples/steps/jumps/2021/11/21/Step-Detection.html",
            "relUrl": "/steps/jumps/2021/11/21/Step-Detection.html",
            "date": " • Nov 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bayes-shape-calc.github.io/examples/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bayes-shape-calc.github.io/examples/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}