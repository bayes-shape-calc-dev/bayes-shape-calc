{
  
    
        "post0": {
            "title": "Signal processing using shapes",
            "content": "import numpy as np import matplotlib.pyplot as plt import scipy as sc import scipy.special import numba as nb from math import lgamma . def evidence(y, x): # corresponds to Eq. 2.2.2 in SI for shape calculations N=float(x.size) ex = np.mean(x) exx = np.mean(x*x) ey = np.mean(y) eyy = np.mean(y*y) exy = np.mean(x*y) vx = exx - ex*ex vy = eyy - ey*ey vxy = exy - ex*ey v2xy = vxy*vxy r2= vxy**2./(vx*vy) m = (N-2.)/2. log_l = -m*np.log(np.pi) log_l += -N/2*np.log(N) log_l += lgamma(m) log_l += -0.5*np.log(vx) log_l += -m*np.log(vy) log_l += -m*np.log(1.-r2) log_l += np.log(1.-.5*sc.special.betainc(m,.5,1.-r2)) return log_l print(evidence(np.random.rand(4), np.random.rand(4))) . 0.7120755761819708 . The data collected is for the fluorescence lifetime of quininine (lit value = 19 ns) in the absence (q00) and in the presence of quenchers (q01-5). The time interval between data points is ~55ps (exact value given below). Each file comes embedded with the experimentally determined IRF (plotted below). . time, irf_q00, decay_q00, _, _ = np.loadtxt(&#39;q00_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . tau = np.arange(0, 3000)*(0.1097394/2) fake_decay = np.exp(-tau/19.49) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, decay_q00) plt.plot(time, 1000*signal/(signal.max())) plt.xlim(15,30) print(evidence(decay_q00, signal)) . -16066.958041718837 . Deconvolution based on known IRF . The first approach is a brute force one. 4000 points are taken between 1 and 100 (ns), and the decay curves corresponding to these points are generated. These decays are convolved with the known IRF to generate templates which are then compared with the shape of the experimental decay using a log-evidence for the shape comparison. If we assume that the prior for the lifetime is uniform, the evidence is proportional to the posterior of the lifetime. This log-evidence is plotted for the 4000 points. The MAP is printed. . Notes: 1) Numpy&#39;s convolve function has three modes, &#39;full&#39;(default), &#39;same&#39;, and &#39;overlap&#39;. These determine the size of the convolved array (see docs for exact description). Ideally, since we need the same size for our template and data, one would assume that the &#39;same&#39; mode is most suitable. However, I tried this and it seems &#39;same&#39; truncates the convolved array in the wrong side (from the beginning and not the end). So I went with convolving in the &#39;full&#39; mode, and then truncating manually. 2) There is a shift parameter in the fitting software, which for these datasets, turns up a value of 3 datapoints. For this comparison, no shift is implemented. If needed, one could marginalise or estimate it. . y_00 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_00.append(evidence(decay_q00, signal)) y_00 = np.array(y_00) plt.figure(1) plt.plot(ts, y_00, &#39;r&#39;) plt.show() print(ts[np.argmax(y_00)]) . 19.492873218304574 . time, irf_q01, decay_q01, _, _ = np.loadtxt(&#39;q01_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_01 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q01, fake_decay)[:time.shape[0]] y_01.append(evidence(decay_q01, signal)) y_01 = np.array(y_01) plt.figure(1) plt.plot(ts, y_01, &#39;r&#39;) plt.show() print(ts[np.argmax(y_01)]) . 8.822955738934734 . time, irf_q02, decay_q02, _, _ = np.loadtxt(&#39;q02_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_02 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_02.append(evidence(decay_q02, signal)) y_02 = np.array(y_02) plt.figure(1) plt.plot(ts, y_02, &#39;r&#39;) plt.show() print(ts[np.argmax(y_02)]) . 5.010502625656414 . time, irf_q03, decay_q03, _, _ = np.loadtxt(&#39;q03_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_03 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_03.append(evidence(decay_q03, signal)) y_03 = np.array(y_03) plt.figure(1) plt.plot(ts, y_03, &#39;r&#39;) plt.show() print(ts[np.argmax(y_03)]) . 7.288072018004501 . time, irf_q04, decay_q04, _, _ = np.loadtxt(&#39;q04_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_04 = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_04.append(evidence(decay_q04, signal)) y_04 = np.array(y_04) plt.figure(1) plt.plot(ts, y_04, &#39;r&#39;) plt.show() print(ts[np.argmax(y_04)]) . 4.441110277569392 . time, irf_q05, decay_q05, _, _ = np.loadtxt(&#39;q05_fit.txt&#39;, skiprows = 58).T time = time*(0.1097394/2) . y_05 = [] y_p = [] tau = time.copy() ts = np.linspace(1,100, 4000) for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] y_05.append(evidence(decay_q05, signal)) y_05 = np.array(y_05) plt.figure(1) plt.plot(ts, y_05, &#39;r&#39;) plt.show() print(ts[np.argmax(y_05)]) . 3.376594148537134 . The above seems to agree with both the known value for quinine and the fits. Now we see what happens when we try to introduce a different IRF to the computation. In this case, I have used a Gaussian IRF centered at 22.5ns and with a sigma of 0.1ns. The comparison with the experimental IRF is given below. . fake_decay = np.exp(-tau/19) signal = np.convolve(irf_q00, fake_decay)[:time.shape[0]] fake_irf = np.zeros_like(irf_q00) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(0.1)**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) #plt.plot(time, decay_q00) #plt.plot(time, signal/25) plt.xlim(15,30) plt.show() print(time[np.argmax(irf_q00)]) . 22.4417073 . y_00_f = [] y_p = [] tau = time.copy() ts = np.linspace(1,100, 4000) #print() for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] y_00_f.append(evidence(decay_q00, signal)) y_00_f = np.array(y_00_f) plt.figure(1) plt.plot(ts, y_00, &#39;k&#39;) plt.plot(ts, y_00_f, &#39;r&#39;) plt.show() print(ts[np.argmax(y_00)]) print(ts[np.argmax(y_00_f)]) . 19.492873218304574 19.715678919729932 . The above curve seems to suggest that M_guess has higher evidence (area under the curve) than M_exp. Not sure how to interpret this beyond: 1) This is partly the effect of the shift parameter. Since the &#39;center&#39; of the experimental IRF is not known, it is probable that the guess IRF is shifted a bit more in the correct direction. 2) This is measured against the quinine data, which has a decay slow enough that the exact form of the IRF is immaterial. . IRF Calibration based on Known Decay . Next up, we try to estimate what the most probable width of the IRF is based on a known value of decay for a signal. For example, the literature value for the fluorescence lifetime of quinine is 19ns. If we use this value to calculate a decay, we want to find what width of IRF gives a signal which is most similar in shape to the experimental signal. Below, the posterior for the sigma of the IRF is shown, the MAP is printed and the IRF corresponding to this MAP sigma is compared to the experimentally determined IRF. . p_irf = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) fake_decay = np.exp(-tau/19.) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_irf.append(evidence(decay_q00, signal)) p_irf = np.array(p_irf) plt.plot(ws, p_irf, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() . 0.39781563126252506 . We see that the inferred IRF looks similar to the experimental IRF. Thus, we can deconvolve in both directions, from a known IRF to an unknown decay, and from a known decay to an unknown IRF. But what happens if both are unknown? . Blind Deconvolution . Next, we see if we can infer both the width and decay from a signal where both are not known. This process is called &#39;blind deconvolution&#39;. Below, we brute force the entire 2D log-posterior for both the sigma and the lifetime. The 1D marginalised posteriors for both are shown, the MAPs are printed and the corresponding IRF/decay signal is compared to the experimental signal. . p_blind = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) ts = np.linspace(1,100, 500) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) p_decay = [] for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_decay.append(evidence(decay_q00, signal)) p_blind.append(p_decay) p_blind = np.array(p_blind) p_irf = p_blind.sum(1) plt.plot(ws, p_irf, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf)])**2)) plt.plot(time, irf_q00, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() p_decay = p_blind.sum(0) plt.plot(ts, p_decay, &#39;r&#39;) plt.show() print(ts[np.argmax(p_decay)]) fake_decay = np.exp(-tau/ts[np.argmax(p_decay)]) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] plt.plot(time, decay_q00, &#39;k&#39;) plt.plot(time, 1000*signal/(signal.max()), &#39;r&#39;) #plt.xlim(18,27) plt.show() . 0.36591182364729463 . 19.649298597194388 . from matplotlib import cm . The surface plot of the 2D posterior is plotted below. . t_g, w_g = np.meshgrid(ts, ws) fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;, &quot;proj_type&quot; : &quot;ortho&quot;}) ax.view_init(30, 45) ax.plot_surface(w_g, t_g, p_blind, cmap = cm.coolwarm, antialiased = &#39;True&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x13274b00730&gt; . The same is done for the dataset &quot;q01&quot;. . p_blind_01 = [] tau = time.copy() ws = np.linspace(0.005, 1, 500) ts = np.linspace(1,100, 500) for w in ws: fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(w)**2)) p_decay_01 = [] for t in ts: fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] p_decay_01.append(evidence(decay_q01, signal)) p_blind_01.append(p_decay_01) p_blind_01 = np.array(p_blind_01) p_irf_01 = p_blind_01.sum(1) plt.plot(ws, p_irf_01, &#39;r&#39;) plt.show() print(ws[np.argmax(p_irf_01)]) fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(ws[np.argmax(p_irf_01)])**2)) plt.plot(time, irf_q01, &#39;k&#39;) plt.plot(time, fake_irf, &#39;r&#39;) plt.xlim(18,27) plt.show() p_decay_01 = p_blind_01.sum(0) plt.plot(ts, p_decay_01, &#39;r&#39;) plt.show() print(ts[np.argmax(p_decay_01)]) fake_decay = np.exp(-tau/ts[np.argmax(p_decay_01)]) signal_01 = np.convolve(fake_irf, fake_decay)[:time.shape[0]] plt.plot(time, decay_q01, &#39;k&#39;) plt.plot(time, 1000*signal_01/(signal_01.max()), &#39;r&#39;) #plt.xlim(18,27) plt.show() . 0.2642184368737475 . 8.935871743486974 . fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;, &quot;proj_type&quot; : &quot;ortho&quot;}) ax.view_init(25, 35) ax.plot_surface(w_g, t_g, p_blind_01, cmap = cm.coolwarm, antialiased = &#39;True&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x132768c96d0&gt; . MCMC Sampling . Brute force grid searching is inefficient and hard to visualize, when the posterior has more than 2 dimensions. Therefore, we move to an MCMC sampling approach. First we show that our above approach of blind deconvolution for each individual signal can be replicated with MCMC. . import emcee as mc import corner as cor from tqdm.notebook import tqdm . The posterior is defined using the evidence function between the guess signal and the experimental signal as the likelihood, and using uniform priors for sigma and lifetime. . sigma ~ Uniform(0., 1.) Lifetime ~ Uniform(0.,100.) . def log_post(param, decay, time): if param[0] &lt;= 0. or param[1] &lt;= 0.: return -np.inf if param[0] &gt; 100. or param[1] &gt; 1.: return -np.inf t1 = param[0] sigma = param[1] fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2)) fake_decay = np.exp(-tau/t1) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] log_l = evidence(decay, signal) return log_l . We define the same sampling condition for all datasets. The MCMC algorithm will use 50 walkers, the same initialisations, and will sample for 5000 steps, after a burn-in of 500 steps. For each dataset, the likelihood for all of the walkers are plotted both during the burn-in and production run, to ensure that the walkers are properly equilibriated during production. . At the end, uncorrrelated samples are collected from the MCMC chains by only accepting samples at the interval of the maximum autocorrelation time of the chains. A corner plot of the histograms of these uncorrelated samples is generated. . ndim = 2 nwalkers = 50 np.random.seed(666) p0 = [np.array([19., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] . sampler0 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q00, time]) . init = 500 for i, result in enumerate(tqdm(sampler0.sample(p0,iterations = init), total = init)): pass p1 = sampler0.chain[:,-1].copy() plt.plot(sampler0.lnprobability.T, alpha = 0.1) plt.show() sampler0.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler0.sample(p1,iterations = init), total = init)): pass plt.plot(sampler0.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples0 = sampler0.get_chain(flat=True)[::int(sampler0.get_autocorr_time().max())] print(samples0.shape) cor.corner(samples0) plt.show() . (8334, 2) . sampler = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q01, time]) init = 500 for i, result in enumerate(tqdm(sampler.sample(p0,iterations = init), total = init)): pass p1 = sampler.chain[:,-1].copy() plt.plot(sampler.lnprobability.T, alpha = 0.1) plt.show() sampler.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler.sample(p1,iterations = init), total = init)): pass plt.plot(sampler.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples = sampler.get_chain(flat=True)[::int(sampler.get_autocorr_time().max())] print(samples.shape) fig = cor.corner(samples) plt.show() . (8334, 2) . sampler2 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q02, time]) init = 500 for i, result in enumerate(tqdm(sampler2.sample(p0,iterations = init), total = init)): pass p1 = sampler2.chain[:,-1].copy() plt.plot(sampler2.lnprobability.T, alpha = 0.1) plt.show() sampler2.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler2.sample(p1,iterations = init), total = init)): pass plt.plot(sampler2.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples2 = sampler2.get_chain(flat=True)[::int(sampler2.get_autocorr_time().max())] print(samples2.shape) fig = cor.corner(samples2) plt.show() . (7813, 2) . sampler3 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q03, time]) init = 500 for i, result in enumerate(tqdm(sampler3.sample(p0,iterations = init), total = init)): pass p1 = sampler3.chain[:,-1].copy() plt.plot(sampler3.lnprobability.T, alpha = 0.1) plt.show() sampler3.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler3.sample(p1,iterations = init), total = init)): pass plt.plot(sampler3.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples3 = sampler3.get_chain(flat=True)[::int(sampler3.get_autocorr_time().max())] print(samples3.shape) fig = cor.corner(samples3) plt.show() . (8065, 2) . sampler4 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q04, time]) init = 500 for i, result in enumerate(tqdm(sampler4.sample(p0,iterations = init), total = init)): pass p1 = sampler4.chain[:,-1].copy() plt.plot(sampler4.lnprobability.T, alpha = 0.1) plt.show() sampler4.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler4.sample(p1,iterations = init), total = init)): pass plt.plot(sampler4.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples4 = sampler4.get_chain(flat=True)[::int(sampler4.get_autocorr_time().max())] print(samples4.shape) fig = cor.corner(samples4) plt.show() . (8334, 2) . sampler5 = mc.EnsembleSampler(nwalkers, ndim, log_post, args=[decay_q05, time]) init = 500 for i, result in enumerate(tqdm(sampler5.sample(p0,iterations = init), total = init)): pass p1 = sampler5.chain[:,-1].copy() plt.plot(sampler5.lnprobability.T, alpha = 0.1) plt.show() sampler5.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler5.sample(p1,iterations = init), total = init)): pass plt.plot(sampler5.lnprobability.T, alpha = 0.1) plt.show() . (50, 2) . samples5 = sampler5.get_chain(flat=True)[::int(sampler5.get_autocorr_time().max())] print(samples5.shape) fig = cor.corner(samples5) plt.show() . (8334, 2) . We see above that the results from the MCMC match the ones of the grid search relatively well. However, interestingly, in both cases, the blind deconvolution of individual signals lead to a slightly different result for the IRF. Ideally, the IRF should be the same for all signals. Can we get this consensus IRF by analysing multiple signals at once? . Consensus Blind Deconvolution . We define a new posterior function which is capable of analysing an arbitrary numbe of decay functions simultaneously. The exact dimensionality of the posterior is determined by the input decay array. . def log_post2(param, decay, time): if len(decay) != param.shape[0] - 1: return np.NaN if np.any(param &lt;= 0.): return -np.inf if np.any(param[:-1] &gt; 100.) or param[-1] &gt; 1.: return -np.inf sigma = param[-1] fake_irf = np.zeros_like(time) fake_irf += 1000*np.exp(-((time - 22.5)**2)/(2*(sigma)**2)) log_l = 0 tau = time for i,t in enumerate(param[:-1]): fake_decay = np.exp(-tau/t) signal = np.convolve(fake_irf, fake_decay)[:time.shape[0]] log_l += evidence(decay[i], signal) return log_l . First, we see if we can model two decays simultaneously. This is a 3-dimensional problem. The code is written so that once the decay list has been constructed, the dimensionality of the problem is set. However, the initialization array still needs to be amended in terms of dimensionality. . decay = [decay_q00, decay_q01] ndim = len(decay) + 1 nwalkers = 50 np.random.seed(666) p0 = [np.array([15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] print(log_post2(p0[0], decay, time)) sampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time]) init = 500 for i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass p1 = sampler_all1.chain[:,-1].copy() plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() sampler_all1.reset() print(p1.shape) init = 5000 for i, result in enumerate(tqdm(sampler_all1.sample(p1,iterations = init), total = init)): pass plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() . -40240.3395576686 . (50, 3) . samples_all1 = sampler_all1.get_chain(flat=True)[::int(sampler_all1.get_autocorr_time().max())] print(samples_all1.shape) fig = cor.corner(samples_all1) plt.show() . (6098, 3) . The MAPs for the decay lifetimes seem to agree with the 2D estimates. As expected, the consensus IRF sigma is estimated to be somewhere between the 2D estimates. The evolution of the likelihoods along the change also indicates that this is not a difficult space for the walkers to explore. . Next, we move on to the problem of consensus blind deconvolution for all 6 decays. As can be seen, this posterior space is difficult to traverse, probably due to the high dimensionality. As a result, we start an initial run of 60 walkers for 2000 steps. Then we use the best 30 positions from those walkers at the end of their run to initialize a new set of 30 walkers. These walkers then have a burn-in of 1000 steps and a production run of 15000 steps. This allows us to pull uncorrelated samples for the posterior. . decay = [decay_q00, decay_q01, decay_q02, decay_q03, decay_q04, decay_q05] ndim = len(decay) + 1 nwalkers = 60 np.random.seed(666) p0 = [np.array([15.,15., 15.,15.,15.,15., 0.1]) + 1e-6*np.random.rand(ndim) for _ in range(nwalkers)] print(log_post2(p0[0], decay, time)) sampler_all1 = mc.EnsembleSampler(nwalkers, ndim, log_post2, args=[decay, time]) init = 2000 for i, result in enumerate(tqdm(sampler_all1.sample(p0,iterations = init), total = init)): pass p1 = sampler_all1.chain[:,-1].copy() plt.plot(sampler_all1.lnprobability.T, alpha = 0.1) plt.show() lx = sampler_all1.lnprobability[-1,:].argsort() p1 = sampler_all1.chain[-1,lx][-int(nwalkers/2):] sampler_all2 = mc.EnsembleSampler(int(nwalkers/2), ndim, log_post2, args=[decay, time]) init = 1000 for i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass p2 = sampler_all2.chain[:,-1].copy() plt.plot(sampler_all2.lnprobability.T, alpha = 0.1) plt.show() sampler_all2.reset() init = 15000 for i, result in enumerate(tqdm(sampler_all2.sample(p1,iterations = init), total = init)): pass plt.plot(sampler_all2.lnprobability.T, alpha = 0.1) plt.show() . -128353.52673677605 . samples_all2 = sampler_all2.get_chain(flat=True)[::int(sampler_all2.get_autocorr_time().max())] print(samples_all2.shape) fig = cor.corner(samples_all2, labels = [r&#39;$ tau_{q00}$&#39;, r&#39;$ tau_{q01}$&#39;, r&#39;$ tau_{q02}$&#39;, r&#39;$ tau_{q03}$&#39;, r&#39;$ tau_{q04}$&#39;, r&#39;$ tau_{q05}$&#39;, r&#39;$ sigma$&#39;]) plt.show() . (5358, 7) .",
            "url": "https://bayes-shape-calc-dev.github.io/bayes-shape-calc/deconvolution/calibration/mcmc/2021/11/18/signal-processing.html",
            "relUrl": "/deconvolution/calibration/mcmc/2021/11/18/signal-processing.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bayes-shape-calc-dev.github.io/bayes-shape-calc/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bayes-shape-calc-dev.github.io/bayes-shape-calc/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bayes-shape-calc-dev.github.io/bayes-shape-calc/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bayes-shape-calc-dev.github.io/bayes-shape-calc/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}